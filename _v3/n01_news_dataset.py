import pandas as pd
import numpy as np
import os
import torch
import glob
import re
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm

class NewsSentimentAnalyzer:
    def __init__(self, keywords_path, model_name="snunlp/KR-FinBert-SC"):
        # 1. í‚¤ì›Œë“œ ë¡œë“œ
        if not os.path.exists(keywords_path):
            raise FileNotFoundError(f"í‚¤ì›Œë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {keywords_path}")
            
        with open(keywords_path, 'r', encoding='utf-8') as f:
            # ë¹ˆ ì¤„ ì œì™¸í•˜ê³  ë¦¬ìŠ¤íŠ¸ì—…
            self.keywords = [line.strip() for line in f if line.strip()]
        
        # í‚¤ì›Œë“œ í•„í„°ë§ìš© ì •ê·œí‘œí˜„ì‹ (íŒ¨í„´ì´ ë„ˆë¬´ ê¸¸ë©´ ì—ëŸ¬ë‚  ìˆ˜ ìˆì–´ ìœ ì˜í•´ì•¼ í•¨)
        self.keyword_pattern = '|'.join([re.escape(k) for k in self.keywords])
        
        # 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        print(f"ğŸ“¦ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        self.model.eval()

    def filter_relevant_news(self, df):
        """í‚¤ì›Œë“œ ì—´ì— ê´€ë ¨ ë‹¨ì–´ê°€ í¬í•¨ëœ ë‰´ìŠ¤ë§Œ í•„í„°ë§"""
        if 'í‚¤ì›Œë“œ' not in df.columns:
            print("âš ï¸ 'í‚¤ì›Œë“œ' ì»¬ëŸ¼ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        # [ìˆ˜ì •] engine ì¸ì ì‚­ì œ ë° regex=True ëª…ì‹œ
        # na=Falseë¡œ ê²°ì¸¡ì¹˜ëŠ” ë¬´ì‹œí•˜ê³  íŒ¨í„´ í¬í•¨ ì—¬ë¶€ í™•ì¸
        mask = df['í‚¤ì›Œë“œ'].str.contains(self.keyword_pattern, na=False, regex=True)
        return df[mask].copy()

    def get_sentiment_scores(self, titles, batch_size=64):
        """ì œëª© ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ê°ì„± ì ìˆ˜ ì‚°ì¶œ (-1 ~ 1)"""
        all_scores = []
        
        # tqdmìœ¼ë¡œ ì§„í–‰ ìƒí™© í‘œì‹œ
        for i in tqdm(range(0, len(titles), batch_size), desc="ê°ì„± ë¶„ì„ ì¤‘"):
            batch_titles = titles[i:i+batch_size]
            inputs = self.tokenizer(batch_titles, return_tensors="pt", padding=True, 
                                    truncation=True, max_length=128).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()
                
                # ì ìˆ˜ ê³„ì‚°: (ê¸ì • í™•ë¥  * 1) + (ë¶€ì • í™•ë¥  * -1)
                # ë¼ë²¨: 0(ë¶€ì •), 1(ì¤‘ë¦½), 2(ê¸ì •)
                scores = (probs[:, 2] * 1) + (probs[:, 0] * -1)
                all_scores.extend(scores)
        
        return all_scores

def run_sentiment_pipeline():
    # ê²½ë¡œ ì„¤ì • (ì¬í˜„ë‹˜ í™˜ê²½ì— ë§ê²Œ í™•ì¸ í•„ìš”)
    news_dir = "D:/stock/_data/news"
    keywords_file = "D:/stock/_v3/_data/refined_keywords.txt" # ì´ íŒŒì¼ì´ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ ê²½ë¡œì— ìˆì–´ì•¼ í•¨
    output_path = "D:/stock/_v3/_data/daily_news_sentiment.csv"
    
    analyzer = NewsSentimentAnalyzer(keywords_file)
    excel_files = glob.glob(os.path.join(news_dir, "*.xlsx"))
    
    if not excel_files:
        print(f"âŒ '{news_dir}' ê²½ë¡œì— ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    daily_results = []

    for file in excel_files:
        print(f"\nğŸ“– íŒŒì¼ ì½ê¸°: {os.path.basename(file)}")
        # í•„ìš”í•œ ì—´ë§Œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
        try:
            df = pd.read_excel(file, usecols=['ì¼ì', 'ì œëª©', 'í‚¤ì›Œë“œ'])
        except Exception as e:
            print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file}): {e}")
            continue
        
        # 1. í‚¤ì›Œë“œ í•„í„°ë§
        filtered_df = analyzer.filter_relevant_news(df)
        if filtered_df.empty:
            print("â© ë§¤ì¹­ë˜ëŠ” í‚¤ì›Œë“œ ë‰´ìŠ¤ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
            
        print(f"âœ… í•„í„°ë§ ì™„ë£Œ: {len(df)}ê±´ ì¤‘ {len(filtered_df)}ê±´ ì„ ë³„")

        # 2. ê°ì„± ì ìˆ˜ ê³„ì‚°
        titles = filtered_df['ì œëª©'].astype(str).tolist()
        filtered_df['sentiment_score'] = analyzer.get_sentiment_scores(titles)

        # 3. ë‚ ì§œ í˜•ì‹ ì •ë¦¬ (YYYYMMDD)
        filtered_df['ì¼ì'] = pd.to_datetime(filtered_df['ì¼ì'].astype(str)).dt.strftime('%Y%m%d')
        
        # 4. ì¼ìë³„ í‰ê·  ì ìˆ˜ ì§‘ê³„
        daily_avg = filtered_df.groupby('ì¼ì')['sentiment_score'].mean().reset_index()
        daily_results.append(daily_avg)

    # ì „ì²´ ê²°ê³¼ í•©ì‚°
    if daily_results:
        final_df = pd.concat(daily_results, ignore_index=True)
        # ê°™ì€ ë‚ ì§œê°€ ì—¬ëŸ¬ íŒŒì¼ì— ê±¸ì³ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ì‹œ í•œë²ˆ í‰ê· 
        final_daily_sentiment = final_df.groupby('ì¼ì')['sentiment_score'].mean().reset_index()
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        final_daily_sentiment.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"\nâœ¨ ìµœì¢… ì¼ë³„ ê°ì„± ì ìˆ˜ ì €ì¥ ì™„ë£Œ: {output_path}")
    else:
        print("âŒ ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_sentiment_pipeline()