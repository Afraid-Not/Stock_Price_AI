import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import os

# [1] ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ìŠ¬ë¼ì´ë”© ìœˆë„ìš°)
class StockDataset(Dataset):
    def __init__(self, data_values, window_size=20):
        """
        data_values: [N, Features + Target] í˜•íƒœì˜ numpy array
        """
        self.data = data_values.astype(np.float32)
        self.window_size = window_size

    def __len__(self):
        return len(self.data) - self.window_size

    def __getitem__(self, idx):
        # x: window_size ë§Œí¼ì˜ í”¼ì²˜ë“¤, y: íƒ€ê²Ÿ ê°’
        x = self.data[idx : idx + self.window_size, :-1]
        y = self.data[idx + self.window_size, -1]
        return torch.tensor(x), torch.tensor(y)

# [2] LSTM ëª¨ë¸ ì •ì˜
class StockLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, num_layers=2):
        super(StockLSTM, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        # x shape: (batch, seq_len, input_dim)
        out, _ = self.lstm(x)
        # ë§ˆì§€ë§‰ ì‹œì (last time step)ì˜ ì¶œë ¥ë§Œ ì‚¬ìš©
        return self.fc(out[:, -1, :]).squeeze()

# [3] ê°€ì¤‘ì¹˜ ì´ì‹ ë° ì°¨ì› í™•ì¥ í•¨ìˆ˜
def expand_and_transfer(pretrained_model, new_input_dim, hidden_dim=64, num_layers=2):
    """ 
    ê¸°ì¡´ ì£¼ê°€ ëª¨ë¸ì˜ ì§€ì‹ì„ ë‰´ìŠ¤ í†µí•© ëª¨ë¸ë¡œ ì´ì‹ 
    """
    new_model = StockLSTM(input_dim=new_input_dim, hidden_dim=hidden_dim, num_layers=num_layers)
    old_dict = pretrained_model.state_dict()
    new_dict = new_model.state_dict()

    for name, param in old_dict.items():
        if 'lstm.weight_ih_l0' in name:
            # ì…ë ¥ ë ˆì´ì–´ ê°€ì¤‘ì¹˜: ê¸°ì¡´ í”¼ì²˜ ë¶€ë¶„ì€ ë³µì‚¬, ì¶”ê°€ëœ ë‰´ìŠ¤ í”¼ì²˜ ë¶€ë¶„ì€ ì´ˆê¸°í™”
            new_dict[name][:, :-1] = param
            nn.init.xavier_normal_(new_dict[name][:, -1:])
        else:
            new_dict[name] = param
            
    new_model.load_state_dict(new_dict)
    return new_model

# [4] í•™ìŠµ ë£¨í”„ í•¨ìˆ˜
def train_model(model, train_loader, epochs, lr, device, phase_name="Training"):
    model.to(device)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    
    print(f"\nğŸš€ {phase_name} ì‹œì‘...")
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
            
            epoch_loss += loss.item()
            
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"Epoch [{epoch+1}/{epochs}] - Loss: {epoch_loss/len(train_loader):.6f}")
            
    return model

def main():
    # ì„¤ì •
    window_size = 20
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    base_dir = "D:/stock/_v3/_data"
    
    # 1. ë°ì´í„° ë¡œë“œ ë° ë¶„ë¦¬
    print("ğŸ“‚ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í•™ìŠµ ë‹¨ê³„ë³„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤...")
    stock_df = pd.read_csv(f"{base_dir}/preprocessed_005930.csv")
    news_df = pd.read_csv(f"{base_dir}/daily_news_sentiment.csv")
    
    # ë‚ ì§œ í˜•ì‹ í†µì¼ (YYYYMMDD)
    stock_df['ë‚ ì§œ'] = stock_df['ë‚ ì§œ'].astype(str).str.replace('-', '')
    news_df['ì¼ì'] = news_df['ì¼ì'].astype(str)
    
    # --- [Phase 1 ë°ì´í„° ì¤€ë¹„] 2010~2021 (ì£¼ê°€ë§Œ) ---
    phase1_df = stock_df[stock_df['ë‚ ì§œ'] < '20220101'].copy()
    # 'ë‚ ì§œ' ì œì™¸, 'target'ì€ ë§ˆì§€ë§‰ì— ìœ„ì¹˜í•´ì•¼ í•¨
    phase1_data = phase1_df.drop(columns=['ë‚ ì§œ']).values 
    
    # --- [Phase 2 ë°ì´í„° ì¤€ë¹„] 2022~2025 (ì£¼ê°€ + ë‰´ìŠ¤) ---
    phase2_stock = stock_df[stock_df['ë‚ ì§œ'] >= '20220101'].copy()
    # ë‰´ìŠ¤ ë³‘í•©
    phase2_combined = pd.merge(phase2_stock, news_df, left_on='ë‚ ì§œ', right_on='ì¼ì', how='left')
    phase2_combined['sentiment_score'] = phase2_combined['sentiment_score'].fillna(0)
    
    # íƒ€ê²Ÿ(target) ì»¬ëŸ¼ì„ ë§¨ ë’¤ë¡œ ë³´ë‚´ê¸° ìœ„í•´ ì¬ë°°ì¹˜
    cols = [c for c in phase2_combined.columns if c not in ['ë‚ ì§œ', 'ì¼ì', 'target']] + ['target']
    phase2_data = phase2_combined[cols].values

    # 2. Phase 1: Pre-training
    train_ds1 = StockDataset(phase1_data, window_size=window_size)
    train_loader1 = DataLoader(train_ds1, batch_size=64, shuffle=True)
    
    input_dim1 = phase1_data.shape[1] - 1 # target ì œì™¸
    base_model = StockLSTM(input_dim=input_dim1)
    
    base_model = train_model(base_model, train_loader1, epochs=30, lr=0.001, device=device, phase_name="Phase 1 (Pre-training)")
    torch.save(base_model.state_dict(), f"{base_dir}/base_model.pth")

    # 3. Phase 2: Fine-tuning (ë‰´ìŠ¤ ì ìˆ˜ ì¶”ê°€)
    train_ds2 = StockDataset(phase2_data, window_size=window_size)
    train_loader2 = DataLoader(train_ds2, batch_size=32, shuffle=True)
    
    input_dim2 = phase2_data.shape[1] - 1
    # ê°€ì¤‘ì¹˜ ì´ì‹ ë° ëª¨ë¸ í™•ì¥
    final_model = expand_and_transfer(base_model, new_input_dim=input_dim2)
    
    # íŒŒì¸íŠœë‹ ì‹œì—ëŠ” ë‚®ì€ í•™ìŠµë¥ (LR) ì‚¬ìš©
    final_model = train_model(final_model, train_loader2, epochs=20, lr=0.0001, device=device, phase_name="Phase 2 (Fine-tuning)")
    torch.save(final_model.state_dict(), f"{base_dir}/final_multimodal_model.pth")

    print(f"\nâœ¨ ëª¨ë“  í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ {base_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()