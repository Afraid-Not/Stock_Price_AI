# -*- coding: utf-8 -*-
"""
kr-finbertë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ê°ì„± ë¶„ì„
LLM ë¶„ì„ ê²°ê³¼ì™€ ë¹„êµìš©
"""

import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import warnings
warnings.filterwarnings('ignore')

class FinBertAnalyzer:
    def __init__(self, model_name: str = "snunlp/KR-FinBert-SC"):
        """
        kr-finbert ëª¨ë¸ ë¡œë“œ
        snunlp/KR-FinBert-SC: í•œêµ­ì–´ ê¸ˆìœµ ê°ì„± ë¶„ì„ ëª¨ë¸
        """
        print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ–¥ï¸ Device: {self.device}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        
        # ë ˆì´ë¸” ë§¤í•‘ (KR-FinBert-SCëŠ” negative, neutral, positive)
        self.label_map = {0: -1.0, 1: 0.0, 2: 1.0}  # negative, neutral, positive
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def analyze_text(self, text: str) -> dict:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„"""
        if not text or pd.isna(text):
            return {'sentiment': 0.0, 'confidence': 0.0, 'label': 'neutral'}
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ìë¥´ê¸°)
        text = str(text)[:512]
        
        try:
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                max_length=512,
                padding=True
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                probs = torch.softmax(outputs.logits, dim=-1)
                pred_label = torch.argmax(probs, dim=-1).item()
                confidence = probs[0][pred_label].item()
            
            # ê°ì„± ì ìˆ˜ ê³„ì‚° (í™•ë¥  ê°€ì¤‘ í‰ê· )
            sentiment_score = (
                probs[0][0].item() * (-1.0) +  # negative
                probs[0][1].item() * 0.0 +     # neutral
                probs[0][2].item() * 1.0       # positive
            )
            
            labels = ['negative', 'neutral', 'positive']
            return {
                'sentiment': sentiment_score,
                'confidence': confidence,
                'label': labels[pred_label]
            }
        except Exception as e:
            return {'sentiment': 0.0, 'confidence': 0.0, 'label': 'neutral'}
    
    def analyze_batch(self, texts: list, batch_size: int = 32) -> list:
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¹ ë¥¸ ë¶„ì„"""
        results = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc="ë¶„ì„ ì¤‘"):
            batch_texts = texts[i:i+batch_size]
            # Noneì´ë‚˜ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
            batch_texts = [str(t)[:512] if t and not pd.isna(t) else "" for t in batch_texts]
            
            try:
                inputs = self.tokenizer(
                    batch_texts,
                    return_tensors="pt",
                    truncation=True,
                    max_length=512,
                    padding=True
                )
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    probs = torch.softmax(outputs.logits, dim=-1)
                
                for j in range(len(batch_texts)):
                    sentiment_score = (
                        probs[j][0].item() * (-1.0) +
                        probs[j][1].item() * 0.0 +
                        probs[j][2].item() * 1.0
                    )
                    pred_label = torch.argmax(probs[j]).item()
                    labels = ['negative', 'neutral', 'positive']
                    
                    results.append({
                        'sentiment': sentiment_score,
                        'confidence': probs[j][pred_label].item(),
                        'label': labels[pred_label]
                    })
            except Exception as e:
                # ì—ëŸ¬ ì‹œ neutralë¡œ ì²˜ë¦¬
                for _ in batch_texts:
                    results.append({'sentiment': 0.0, 'confidence': 0.0, 'label': 'neutral'})
        
        return results


def analyze_news_file(news_path: str, output_dir: str = "_data/news_sentiment_finbert"):
    """ë‰´ìŠ¤ íŒŒì¼ì„ kr-finbertë¡œ ë¶„ì„"""
    
    # íŒŒì¼ ë¡œë“œ
    print(f"\nğŸ“‚ ë‰´ìŠ¤ íŒŒì¼ ë¡œë”©: {news_path}")
    df = pd.read_csv(news_path, encoding='utf-8')
    print(f"   ì´ ë‰´ìŠ¤: {len(df):,}ê±´")
    
    stock_code = Path(news_path).stem.split('_')[1]
    
    # FinBert ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = FinBertAnalyzer()
    
    # ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
    titles = df['HTS_ê³µì‹œ_ì œëª©_ë‚´ìš©'].tolist()
    
    # ë°°ì¹˜ ë¶„ì„
    print("\nğŸ” ê°ì„± ë¶„ì„ ì‹œì‘...")
    results = analyzer.analyze_batch(titles, batch_size=32)
    
    # ê²°ê³¼ ì¶”ê°€
    df['finbert_sentiment'] = [r['sentiment'] for r in results]
    df['finbert_confidence'] = [r['confidence'] for r in results]
    df['finbert_label'] = [r['label'] for r in results]
    
    # ë‚ ì§œ ì»¬ëŸ¼ ìƒì„±
    df['ë‚ ì§œ'] = pd.to_datetime(df['ì‘ì„±ì¼ì'], format='%Y%m%d')
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ê°œë³„ ê²°ê³¼ ì €ì¥
    sentiment_file = output_path / f"finbert_{stock_code}.csv"
    df.to_csv(sentiment_file, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ ê°œë³„ ë¶„ì„ ì €ì¥: {sentiment_file}")
    
    # ì¼ë³„ ì§‘ê³„
    daily_df = df.groupby('ë‚ ì§œ').agg({
        'finbert_sentiment': 'mean',
        'finbert_confidence': 'mean',
        'finbert_label': lambda x: x.value_counts().index[0] if len(x) > 0 else 'neutral',  # ìµœë¹ˆê°’
        'stock_code': 'first'
    }).reset_index()
    daily_df['news_count'] = df.groupby('ë‚ ì§œ').size().values
    daily_df = daily_df.rename(columns={'finbert_sentiment': 'news_sentiment'})
    
    daily_file = output_path / f"daily_finbert_{stock_code}.csv"
    daily_df.to_csv(daily_file, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ ì¼ë³„ ì§‘ê³„ ì €ì¥: {daily_file}")
    
    return df, daily_df


def compare_with_llm(stock_code: str = "000660"):
    """LLM ê²°ê³¼ì™€ FinBert ê²°ê³¼ ë¹„êµ"""
    
    llm_path = f"_data/news_sentiment/daily_{stock_code}.csv"
    finbert_path = f"_data/news_sentiment_finbert/daily_finbert_{stock_code}.csv"
    
    print("\n" + "="*60)
    print("ğŸ“Š LLM vs FinBert ë¹„êµ ë¶„ì„")
    print("="*60)
    
    # LLM ê²°ê³¼ ë¡œë“œ
    llm_df = pd.read_csv(llm_path)
    llm_df['ë‚ ì§œ'] = pd.to_datetime(llm_df['ë‚ ì§œ'])
    
    # FinBert ê²°ê³¼ ë¡œë“œ
    finbert_df = pd.read_csv(finbert_path)
    finbert_df['ë‚ ì§œ'] = pd.to_datetime(finbert_df['ë‚ ì§œ'])
    
    # ë³‘í•©
    merged = pd.merge(
        llm_df[['ë‚ ì§œ', 'news_sentiment', 'news_count']],
        finbert_df[['ë‚ ì§œ', 'news_sentiment', 'finbert_confidence']],
        on='ë‚ ì§œ',
        suffixes=('_llm', '_finbert')
    )
    
    # ìƒê´€ê´€ê³„ ë¶„ì„
    correlation = merged['news_sentiment_llm'].corr(merged['news_sentiment_finbert'])
    
    print(f"\nğŸ“ˆ ë¶„ì„ ê²°ê³¼:")
    print(f"   - ë¹„êµ ì¼ìˆ˜: {len(merged)}ì¼")
    print(f"   - ìƒê´€ê³„ìˆ˜: {correlation:.4f}")
    
    # ê¸°ë³¸ í†µê³„
    print(f"\nğŸ“Š LLM ê°ì„± í†µê³„:")
    print(f"   - í‰ê· : {merged['news_sentiment_llm'].mean():.4f}")
    print(f"   - í‘œì¤€í¸ì°¨: {merged['news_sentiment_llm'].std():.4f}")
    print(f"   - ìµœì†Œ/ìµœëŒ€: {merged['news_sentiment_llm'].min():.4f} / {merged['news_sentiment_llm'].max():.4f}")
    
    print(f"\nğŸ“Š FinBert ê°ì„± í†µê³„:")
    print(f"   - í‰ê· : {merged['news_sentiment_finbert'].mean():.4f}")
    print(f"   - í‘œì¤€í¸ì°¨: {merged['news_sentiment_finbert'].std():.4f}")
    print(f"   - ìµœì†Œ/ìµœëŒ€: {merged['news_sentiment_finbert'].min():.4f} / {merged['news_sentiment_finbert'].max():.4f}")
    
    # ë°©í–¥ ì¼ì¹˜ìœ¨ (ë¶€í˜¸ê°€ ê°™ì€ ë¹„ìœ¨)
    same_direction = ((merged['news_sentiment_llm'] * merged['news_sentiment_finbert']) >= 0).sum()
    direction_rate = same_direction / len(merged) * 100
    print(f"\nğŸ¯ ë°©í–¥ ì¼ì¹˜ìœ¨: {direction_rate:.1f}% ({same_direction}/{len(merged)})")
    
    # ë¹„êµ ê²°ê³¼ ì €ì¥
    comparison_file = f"_data/news_sentiment_finbert/comparison_{stock_code}.csv"
    merged.to_csv(comparison_file, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥: {comparison_file}")
    
    # ì‹œê°ì ìœ¼ë¡œ ëª‡ ê°œ ìƒ˜í”Œ ë¹„êµ
    print("\n" + "="*60)
    print("ğŸ“ ìƒ˜í”Œ ë¹„êµ (ê°ì„± ì°¨ì´ê°€ í° ë‚ )")
    print("="*60)
    merged['diff'] = abs(merged['news_sentiment_llm'] - merged['news_sentiment_finbert'])
    top_diff = merged.nlargest(5, 'diff')
    
    for _, row in top_diff.iterrows():
        print(f"\në‚ ì§œ: {row['ë‚ ì§œ'].strftime('%Y-%m-%d')}")
        print(f"   LLM: {row['news_sentiment_llm']:+.3f}")
        print(f"   FinBert: {row['news_sentiment_finbert']:+.3f}")
        print(f"   ì°¨ì´: {row['diff']:.3f}")
    
    return merged, correlation


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='kr-finbert ë‰´ìŠ¤ ê°ì„± ë¶„ì„')
    parser.add_argument('--stock', type=str, default='000660', help='ì¢…ëª©ì½”ë“œ')
    parser.add_argument('--compare', action='store_true', help='LLMê³¼ ë¹„êµ')
    args = parser.parse_args()
    
    news_file = f"_data/news/news_{args.stock}_20250203_20260203.csv"
    
    # FinBert ë¶„ì„ ì‹¤í–‰
    df, daily_df = analyze_news_file(news_file)
    
    # LLMê³¼ ë¹„êµ
    if args.compare:
        compare_with_llm(args.stock)
    
    print("\nâœ… ì™„ë£Œ!")

