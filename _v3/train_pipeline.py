import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from sklearn.model_selection import TimeSeriesSplit
import os
import copy
import joblib
from sklearn.preprocessing import StandardScaler

# [1] ë°ì´í„°ì…‹ ë° [2] ëª¨ë¸ ì •ì˜ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€
class StockDataset(Dataset):
    def __init__(self, data_values, window_size=20):
        self.data = data_values.astype(np.float32)
        self.window_size = window_size
    def __len__(self):
        return len(self.data) - self.window_size
    def __getitem__(self, idx):
        x = self.data[idx : idx + self.window_size, :-1]
        y = self.data[idx + self.window_size, -1]
        return torch.tensor(x), torch.tensor(y)

class StockLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim=64, num_layers=2):
        super(StockLSTM, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.3)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, 1)
        )
    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :]).squeeze()

def expand_and_transfer(pretrained_model, new_input_dim, hidden_dim=64, num_layers=2):
    new_model = StockLSTM(input_dim=new_input_dim, hidden_dim=hidden_dim, num_layers=num_layers)
    old_dict = pretrained_model.state_dict()
    new_dict = new_model.state_dict()
    for name, param in old_dict.items():
        if 'lstm.weight_ih_l0' in name:
            new_dict[name][:, :-1] = param
            nn.init.xavier_normal_(new_dict[name][:, -1:])
        else:
            new_dict[name] = param
    new_model.load_state_dict(new_dict)
    return new_model

# [4] í•™ìŠµ ë£¨í”„ (êµì°¨ ê²€ì¦ì„ ìœ„í•´ ë¡œê·¸ ì¶œë ¥ ìµœì í™”)
def train_model_with_val(model, train_loader, val_loader, epochs, lr, device, patience=10, verbose=True):
    model.to(device)
    criterion = nn.MSELoss()
    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
    
    best_loss = float('inf')
    best_model_wts = copy.deepcopy(model.state_dict())
    early_stop_counter = 0
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        for x, y in train_loader:
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for x_v, y_v in val_loader:
                x_v, y_v = x_v.to(device), y_v.to(device)
                output_v = model(x_v)
                val_loss += criterion(output_v, y_v).item()
        
        avg_val_loss = val_loss / len(val_loader)
        if avg_val_loss < best_loss:
            best_loss = avg_val_loss
            best_model_wts = copy.deepcopy(model.state_dict())
            early_stop_counter = 0
        else:
            early_stop_counter += 1
            
        if early_stop_counter >= patience:
            break
            
    model.load_state_dict(best_model_wts)
    return model, best_loss

def main():
    window_size = 20
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    base_dir = "D:/stock/_v3/_data"
    n_splits = 5
    tscv = TimeSeriesSplit(n_splits=n_splits)
    
    # ë°ì´í„° ë¡œë“œ
    stock_df = pd.read_csv(f"{base_dir}/preprocessed_005930_20100101_20251231.csv")
    news_df = pd.read_csv(f"{base_dir}/daily_news_sentiment.csv")
    stock_df['ë‚ ì§œ'] = stock_df['ë‚ ì§œ'].astype(str).str.replace('-', '')
    news_df['ì¼ì'] = news_df['ì¼ì'].astype(str)
    
    # --- Phase 1: Pre-training (2010~2021) êµì°¨ ê²€ì¦ ---
    print(f"\nğŸš€ Phase 1 (Pre-training) {n_splits}-Fold êµì°¨ ê²€ì¦ ì‹œì‘...")
    phase1_df = stock_df[stock_df['ë‚ ì§œ'] < '20220101'].copy()
    phase1_values = phase1_df.drop(columns=['ë‚ ì§œ']).values
    input_dim1 = phase1_values.shape[1] - 1
    
    best_base_model = None
    phase1_losses = []

    for fold, (train_idx, val_idx) in enumerate(tscv.split(phase1_values)):
        train_v, val_v = phase1_values[train_idx], phase1_values[val_idx]
        train_loader = DataLoader(StockDataset(train_v, window_size), batch_size=64, shuffle=False)
        val_loader = DataLoader(StockDataset(val_v, window_size), batch_size=64, shuffle=False)
        
        model = StockLSTM(input_dim=input_dim1)
        model, b_loss = train_model_with_val(model, train_loader, val_loader, 100, 0.005, device, 20)
        phase1_losses.append(b_loss)
        print(f" Fold {fold+1}: Best Val Loss = {b_loss:.6f}")
        
        # ë§ˆì§€ë§‰ í´ë“œì˜ ëª¨ë¸ì„ Phase 2ë¡œ ì „ë‹¬í•˜ê¸° ìœ„í•´ ì €ì¥
        if fold == n_splits - 1:
            best_base_model = copy.deepcopy(model)

    print(f"ğŸ“Š Phase 1 í‰ê·  Val Loss: {np.mean(phase1_losses):.6f}")
    torch.save(best_base_model.state_dict(), f"{base_dir}/base_model_cv.pth")

    # --- Phase 2: Fine-tuning (2022~2025) ë°ì´í„° ì¤€ë¹„ ---
    print("\nğŸ“Š ë‰´ìŠ¤ ë°ì´í„° ë³‘í•© ë° ìŠ¤ì¼€ì¼ë§ ì¤‘...")
    phase2_stock = stock_df[stock_df['ë‚ ì§œ'] >= '20220101'].copy()
    phase2_combined = pd.merge(phase2_stock, news_df, left_on='ë‚ ì§œ', right_on='ì¼ì', how='left').fillna(0)
    phase2_combined['sentiment_score'] = phase2_combined['sentiment_score'].shift(1).fillna(0)
    
    s_scaler = StandardScaler()
    phase2_combined['sentiment_score'] = s_scaler.fit_transform(phase2_combined[['sentiment_score']])
    joblib.dump(s_scaler, "D:/stock/_v3/scalers/sentiment_scaler.bin")

    cols = [c for c in phase2_combined.columns if c not in ['ë‚ ì§œ', 'ì¼ì', 'target']] + ['target']
    phase2_values = phase2_combined[cols].values
    input_dim2 = phase2_values.shape[1] - 1

    # --- Phase 2: Fine-tuning êµì°¨ ê²€ì¦ ---
    print(f"ğŸš€ Phase 2 (Fine-tuning) {n_splits}-Fold êµì°¨ ê²€ì¦ ì‹œì‘...")
    phase2_losses = []
    final_model = None

    for fold, (train_idx, val_idx) in enumerate(tscv.split(phase2_values)):
        train_v, val_v = phase2_values[train_idx], phase2_values[val_idx]
        train_loader = DataLoader(StockDataset(train_v, window_size), batch_size=32, shuffle=False)
        val_loader = DataLoader(StockDataset(val_v, window_size), batch_size=32, shuffle=False)
        
        # ê° í´ë“œë§ˆë‹¤ Phase 1ì˜ ê²°ê³¼ë¬¼ë¡œë¶€í„° ìƒˆë¡œ ì‹œì‘
        model = expand_and_transfer(best_base_model, new_input_dim=input_dim2)
        model, b_loss = train_model_with_val(model, train_loader, val_loader, 50, 0.0005, device, 10)
        phase2_losses.append(b_loss)
        print(f" Fold {fold+1}: Best Val Loss = {b_loss:.6f}")
        
        if fold == n_splits - 1:
            final_model = copy.deepcopy(model)

    print(f"ğŸ“Š Phase 2 í‰ê·  Val Loss: {np.mean(phase2_losses):.6f}")
    torch.save(final_model.state_dict(), f"{base_dir}/final_multimodal_model_cv.pth")
    print(f"\ní•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()