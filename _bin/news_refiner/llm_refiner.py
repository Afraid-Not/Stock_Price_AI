"""
LLM(GPT/Claude/Ollama)ì„ ì‚¬ìš©í•œ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ëª¨ë“ˆ
- ê°ì„± ì ìˆ˜ (-1 ~ +1)
- ì˜í–¥ ê°•ë„ (1~5)
- ì´ë²¤íŠ¸ ìœ í˜• ì¶”ì¶œ
"""
import pandas as pd
import numpy as np
import json
import os
import time
from typing import List, Dict, Optional
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ ë¡œë“œ
    env_paths = [
        'D:/stock/.env',  # ì¬í˜„ë‹˜ .env ìœ„ì¹˜
        os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env'),
        '.env',
    ]
    for env_path in env_paths:
        if os.path.exists(env_path):
            load_dotenv(env_path)
            print(f"âœ… .env íŒŒì¼ ë¡œë“œ: {env_path}")
            break
    else:
        load_dotenv()  # ê¸°ë³¸ ìœ„ì¹˜
except ImportError:
    print("âš ï¸ python-dotenv íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. pip install python-dotenv")


class LLMNewsRefiner:
    """LLMì„ ì‚¬ìš©í•œ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, provider: str = "openai", model: str = None, api_key: str = None):
        """
        LLM ì´ˆê¸°í™”
        
        Args:
            provider: "openai", "anthropic", "ollama" ì¤‘ ì„ íƒ
            model: ëª¨ë¸ëª… (ê¸°ë³¸ê°’: providerë³„ ê¸°ë³¸ ëª¨ë¸)
            api_key: API í‚¤ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ë¡œë“œ ê°€ëŠ¥)
        """
        self.provider = provider
        self.client = None
        
        if provider == "openai":
            self.model = model or "gpt-4o-mini"  # ë¹„ìš© íš¨ìœ¨ì 
            self._init_openai(api_key)
        elif provider == "anthropic":
            self.model = model or "claude-3-haiku-20240307"  # ë¹„ìš© íš¨ìœ¨ì 
            self._init_anthropic(api_key)
        elif provider == "ollama":
            self.model = model or "llama3.1"  # ë¡œì»¬ ë¬´ë£Œ
            self._init_ollama()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” provider: {provider}")
    
    def _init_openai(self, api_key: str = None):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            from openai import OpenAI
            api_key = api_key or os.environ.get("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.client = OpenAI(api_key=api_key)
            print(f"âœ… OpenAI ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")
        except ImportError:
            print("âŒ openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   pip install openai")
            raise
    
    def _init_anthropic(self, api_key: str = None):
        """Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            import anthropic
            api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.client = anthropic.Anthropic(api_key=api_key)
            print(f"âœ… Anthropic ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")
        except ImportError:
            print("âŒ anthropic íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("   pip install anthropic")
            raise
    
    def _init_ollama(self):
        """Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë¡œì»¬)"""
        try:
            import requests
            # Ollama ì„œë²„ í™•ì¸
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                print(f"âœ… Ollama ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")
                self.client = "ollama"
            else:
                raise ConnectionError("Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
            print("   ollama serve ëª…ë ¹ìœ¼ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”.")
            raise
    
    def _create_prompt(self, news_title: str, stock_name: str = None) -> str:
        """ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        stock_context = f"'{stock_name}' ì¢…ëª©ì— ëŒ€í•œ " if stock_name else ""
        
        prompt = f"""ë‹¤ìŒì€ {stock_context}ë‰´ìŠ¤ ì œëª©ì…ë‹ˆë‹¤. ì£¼ê°€ì— ë¯¸ì¹  ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ì œëª©: "{news_title}"

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "sentiment": 0.0,      // ê°ì„± ì ìˆ˜ (-1.0 ~ +1.0, ì†Œìˆ˜ì  2ìë¦¬)
    "impact": 3,           // ì˜í–¥ ê°•ë„ (1~5, ì •ìˆ˜)
    "event_type": "ì¼ë°˜",  // ì´ë²¤íŠ¸ ìœ í˜•
    "reason": "ë¶„ì„ ì´ìœ "  // í•œ ì¤„ ì„¤ëª…
}}

ì´ë²¤íŠ¸ ìœ í˜• ëª©ë¡:
- ì‹¤ì : ì‹¤ì  ë°œí‘œ, ë§¤ì¶œ, ì˜ì—…ì´ìµ ê´€ë ¨
- ë°°ë‹¹: ë°°ë‹¹ê¸ˆ, ë°°ë‹¹ ì •ì±… ê´€ë ¨
- ê³„ì•½: ëŒ€ê·œëª¨ ê³„ì•½, ìˆ˜ì£¼, ê³µê¸‰ ê³„ì•½
- íˆ¬ì: ì‹œì„¤ íˆ¬ì, R&D, ì¸ìˆ˜í•©ë³‘
- ì¸ì‚¬: ê²½ì˜ì§„ ë³€ë™, ì¡°ì§ ê°œí¸
- ê·œì œ: ì •ë¶€ ì •ì±…, ê·œì œ, ë²•ì  ì´ìŠˆ
- ì‹œì¥: ì—…í™©, ê²½ìŸ, ì‹œì¥ íŠ¸ë Œë“œ
- ì¼ë°˜: ê¸°íƒ€

ê°ì„± ì ìˆ˜ ê¸°ì¤€:
- +0.8 ~ +1.0: ë§¤ìš° ê¸ì • (ì‚¬ìƒ ìµœëŒ€ ì‹¤ì , ëŒ€ê·œëª¨ ê³„ì•½ ë“±)
- +0.4 ~ +0.7: ê¸ì • (ì‹¤ì  ê°œì„ , ì‹ ì‚¬ì—… ì§„ì¶œ ë“±)
- -0.3 ~ +0.3: ì¤‘ë¦½ (ì¼ë°˜ ë‰´ìŠ¤, ì˜í–¥ ë¶ˆí™•ì‹¤)
- -0.7 ~ -0.4: ë¶€ì • (ì‹¤ì  ì•…í™”, ì†Œì†¡ ë“±)
- -1.0 ~ -0.8: ë§¤ìš° ë¶€ì • (ëŒ€ê·œëª¨ ì†ì‹¤, ì¤‘ëŒ€ ì‚¬ê³  ë“±)

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”:"""
        
        return prompt
    
    def _parse_response(self, response_text: str) -> Dict:
        """LLM ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì¶”ì¶œ ì‹œë„
            # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            if start_idx != -1 and end_idx > start_idx:
                json_str = response_text[start_idx:end_idx]
                result = json.loads(json_str)
                
                # ê°’ ê²€ì¦ ë° í´ë¦¬í•‘
                result['sentiment'] = max(-1.0, min(1.0, float(result.get('sentiment', 0))))
                result['impact'] = max(1, min(5, int(result.get('impact', 3))))
                result['event_type'] = result.get('event_type', 'ì¼ë°˜')
                result['reason'] = result.get('reason', '')
                
                return result
        except (json.JSONDecodeError, ValueError, KeyError) as e:
            print(f"âš ï¸ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        # ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            'sentiment': 0.0,
            'impact': 3,
            'event_type': 'ì¼ë°˜',
            'reason': 'íŒŒì‹± ì‹¤íŒ¨'
        }
    
    def analyze_single(self, news_title: str, stock_name: str = None) -> Dict:
        """ë‹¨ì¼ ë‰´ìŠ¤ ë¶„ì„"""
        prompt = self._create_prompt(news_title, stock_name)
        
        try:
            if self.provider == "openai":
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.1,
                    max_tokens=200
                )
                response_text = response.choices[0].message.content
                
            elif self.provider == "anthropic":
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=200,
                    messages=[{"role": "user", "content": prompt}]
                )
                response_text = response.content[0].text
                
            elif self.provider == "ollama":
                import requests
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {"temperature": 0.1}
                    },
                    timeout=30
                )
                response_text = response.json()['response']
            
            return self._parse_response(response_text)
            
        except Exception as e:
            print(f"âš ï¸ API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return {
                'sentiment': 0.0,
                'impact': 3,
                'event_type': 'ì¼ë°˜',
                'reason': f'API ì˜¤ë¥˜: {str(e)}'
            }
    
    def analyze_batch(self, news_list: List[Dict], stock_name: str = None, 
                      delay: float = 0.5, show_progress: bool = True) -> List[Dict]:
        """
        ë°°ì¹˜ ë‰´ìŠ¤ ë¶„ì„
        
        Args:
            news_list: [{"title": "ë‰´ìŠ¤ì œëª©", "date": "2026-01-01"}, ...]
            stock_name: ì¢…ëª©ëª…
            delay: API í˜¸ì¶œ ê°„ê²© (ì´ˆ)
            show_progress: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        total = len(news_list)
        
        for i, news in enumerate(news_list):
            title = news.get('title', '')
            date = news.get('date', '')
            
            if not title:
                results.append({
                    'date': date,
                    'title': title,
                    'sentiment': 0.0,
                    'impact': 1,
                    'event_type': 'ì¼ë°˜',
                    'reason': 'ì œëª© ì—†ìŒ'
                })
                continue
            
            # ë¶„ì„
            analysis = self.analyze_single(title, stock_name)
            analysis['date'] = date
            analysis['title'] = title
            results.append(analysis)
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if show_progress and (i + 1) % 10 == 0:
                print(f"  ì§„í–‰ë¥ : {i + 1}/{total} ({(i + 1) / total * 100:.1f}%)")
            
            # API ì†ë„ ì œí•œ ë°©ì§€
            if delay > 0 and i < total - 1:
                time.sleep(delay)
        
        return results
    
    def analyze_dataframe(self, df: pd.DataFrame, 
                          title_column: str = "ì œëª©",
                          date_column: str = "ë‚ ì§œ",
                          stock_name: str = None,
                          delay: float = 0.5) -> pd.DataFrame:
        """
        DataFrame ë¶„ì„
        
        Args:
            df: ë‰´ìŠ¤ DataFrame
            title_column: ì œëª© ì»¬ëŸ¼ëª…
            date_column: ë‚ ì§œ ì»¬ëŸ¼ëª…
            stock_name: ì¢…ëª©ëª…
            delay: API í˜¸ì¶œ ê°„ê²©
            
        Returns:
            ë¶„ì„ ê²°ê³¼ê°€ ì¶”ê°€ëœ DataFrame
        """
        print(f"\nğŸ“Š LLM ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘")
        print(f"   ì´ {len(df)}ê±´ì˜ ë‰´ìŠ¤")
        print(f"   ëª¨ë¸: {self.provider}/{self.model}")
        print(f"   ì˜ˆìƒ ì‹œê°„: {len(df) * delay / 60:.1f}ë¶„\n")
        
        # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        news_list = []
        for _, row in df.iterrows():
            news_list.append({
                'title': str(row.get(title_column, '')),
                'date': str(row.get(date_column, ''))
            })
        
        # ë°°ì¹˜ ë¶„ì„
        results = self.analyze_batch(news_list, stock_name, delay)
        
        # ê²°ê³¼ë¥¼ DataFrameì— ì¶”ê°€
        df_result = df.copy()
        df_result['llm_sentiment'] = [r['sentiment'] for r in results]
        df_result['llm_impact'] = [r['impact'] for r in results]
        df_result['llm_event_type'] = [r['event_type'] for r in results]
        df_result['llm_reason'] = [r['reason'] for r in results]
        
        # ê²°ê³¼ í†µê³„
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
        print(f"   í‰ê·  ê°ì„±: {df_result['llm_sentiment'].mean():.3f}")
        print(f"   ê¸ì • ë‰´ìŠ¤: {len(df_result[df_result['llm_sentiment'] > 0.3])}ê±´")
        print(f"   ë¶€ì • ë‰´ìŠ¤: {len(df_result[df_result['llm_sentiment'] < -0.3])}ê±´")
        
        # ì´ë²¤íŠ¸ ìœ í˜• ë¶„í¬
        print(f"\n   ì´ë²¤íŠ¸ ìœ í˜• ë¶„í¬:")
        for event_type, count in df_result['llm_event_type'].value_counts().items():
            print(f"   - {event_type}: {count}ê±´")
        
        return df_result
    
    def aggregate_daily(self, df: pd.DataFrame, 
                        date_column: str = "ë‚ ì§œ",
                        sentiment_column: str = "llm_sentiment",
                        impact_column: str = "llm_impact") -> pd.DataFrame:
        """
        ë‚ ì§œë³„ ê°ì„± ì ìˆ˜ ì§‘ê³„
        
        Args:
            df: ë¶„ì„ëœ DataFrame
            date_column: ë‚ ì§œ ì»¬ëŸ¼ëª…
            sentiment_column: ê°ì„± ì ìˆ˜ ì»¬ëŸ¼ëª…
            impact_column: ì˜í–¥ ê°•ë„ ì»¬ëŸ¼ëª…
            
        Returns:
            ë‚ ì§œë³„ ì§‘ê³„ DataFrame
        """
        # ì˜í–¥ ê°•ë„ë¡œ ê°€ì¤‘ í‰ê· 
        df_temp = df.copy()
        df_temp['weighted_sentiment'] = df_temp[sentiment_column] * df_temp[impact_column]
        
        # ë‚ ì§œë³„ ì§‘ê³„
        df_daily = df_temp.groupby(date_column).agg({
            'weighted_sentiment': 'sum',
            impact_column: 'sum',
            sentiment_column: ['mean', 'count']
        }).reset_index()
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        df_daily.columns = [date_column, 'weighted_sum', 'impact_sum', 'sentiment_mean', 'news_count']
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        df_daily['sentiment_weighted'] = df_daily['weighted_sum'] / df_daily['impact_sum']
        
        # ìµœì¢… ì •ë¦¬
        df_daily = df_daily[[date_column, 'sentiment_weighted', 'sentiment_mean', 'news_count']]
        df_daily = df_daily.rename(columns={
            'sentiment_weighted': 'sentiment_score',
            'sentiment_mean': 'sentiment_simple'
        })
        
        # ì†Œìˆ˜ì  ì •ë¦¬
        df_daily['sentiment_score'] = df_daily['sentiment_score'].round(4)
        df_daily['sentiment_simple'] = df_daily['sentiment_simple'].round(4)
        
        # ë‚ ì§œ ì •ë ¬
        df_daily = df_daily.sort_values(date_column).reset_index(drop=True)
        
        print(f"\nğŸ“Š ë‚ ì§œë³„ ì§‘ê³„ ì™„ë£Œ")
        print(f"   ì´ {len(df_daily)}ì¼")
        print(f"   í‰ê·  ê°ì„± (ê°€ì¤‘): {df_daily['sentiment_score'].mean():.4f}")
        print(f"   í‰ê·  ê°ì„± (ë‹¨ìˆœ): {df_daily['sentiment_simple'].mean():.4f}")
        
        return df_daily


def process_news_with_llm(
    input_path: str,
    output_path: str = None,
    provider: str = "openai",
    model: str = None,
    api_key: str = None,
    title_column: str = "HTS_ê³µì‹œ_ì œëª©_ë‚´ìš©",
    date_column: str = "ì‘ì„±ì¼ì",
    stock_name: str = None,
    delay: float = 0.5,
    aggregate: bool = True
) -> pd.DataFrame:
    """
    ë‰´ìŠ¤ íŒŒì¼ì„ LLMìœ¼ë¡œ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        input_path: ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ
        output_path: ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ
        provider: LLM ì œê³µì (openai, anthropic, ollama)
        model: ëª¨ë¸ëª…
        api_key: API í‚¤
        title_column: ì œëª© ì»¬ëŸ¼ëª…
        date_column: ë‚ ì§œ ì»¬ëŸ¼ëª…
        stock_name: ì¢…ëª©ëª…
        delay: API í˜¸ì¶œ ê°„ê²© (ì´ˆ)
        aggregate: ë‚ ì§œë³„ ì§‘ê³„ ì—¬ë¶€
        
    Returns:
        ë¶„ì„ ê²°ê³¼ DataFrame
    """
    # íŒŒì¼ ì½ê¸°
    print(f"ğŸ“‚ íŒŒì¼ ì½ê¸°: {input_path}")
    try:
        df = pd.read_csv(input_path, encoding='utf-8-sig')
    except:
        df = pd.read_csv(input_path, encoding='cp949')
    print(f"   ì´ {len(df)}ê±´ ë¡œë“œ")
    
    # LLM ì´ˆê¸°í™”
    refiner = LLMNewsRefiner(provider=provider, model=model, api_key=api_key)
    
    # ë¶„ì„
    df_result = refiner.analyze_dataframe(
        df,
        title_column=title_column,
        date_column=date_column,
        stock_name=stock_name,
        delay=delay
    )
    
    # ê²°ê³¼ ì €ì¥
    if output_path is None:
        base_name = os.path.splitext(input_path)[0]
        output_path = f"{base_name}_llm.csv"
    
    df_result.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # ë‚ ì§œë³„ ì§‘ê³„
    if aggregate:
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜
        if date_column in df_result.columns:
            dates = df_result[date_column].astype(str)
            df_result['ë‚ ì§œ'] = dates.apply(
                lambda x: f"{x[:4]}-{x[4:6]}-{x[6:8]}" if len(x) == 8 and x.isdigit() else x
            )
        
        df_daily = refiner.aggregate_daily(df_result, date_column='ë‚ ì§œ')
        
        daily_path = output_path.replace('.csv', '_daily.csv')
        df_daily.to_csv(daily_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ì¼ë³„ ì§‘ê³„ ì €ì¥: {daily_path}")
        
        return df_daily
    
    return df_result


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="LLM ê¸°ë°˜ ë‰´ìŠ¤ ê°ì„± ë¶„ì„")
    parser.add_argument("-i", "--input", type=str, required=True, help="ì…ë ¥ CSV íŒŒì¼")
    parser.add_argument("-o", "--output", type=str, default=None, help="ì¶œë ¥ CSV íŒŒì¼")
    parser.add_argument("--provider", type=str, default="openai", 
                        choices=["openai", "anthropic", "ollama"], help="LLM ì œê³µì")
    parser.add_argument("--model", type=str, default=None, help="ëª¨ë¸ëª…")
    parser.add_argument("--api-key", type=str, default=None, help="API í‚¤")
    parser.add_argument("--title-column", type=str, default="HTS_ê³µì‹œ_ì œëª©_ë‚´ìš©", help="ì œëª© ì»¬ëŸ¼ëª…")
    parser.add_argument("--date-column", type=str, default="ì‘ì„±ì¼ì", help="ë‚ ì§œ ì»¬ëŸ¼ëª…")
    parser.add_argument("--stock-name", type=str, default=None, help="ì¢…ëª©ëª…")
    parser.add_argument("--delay", type=float, default=0.5, help="API í˜¸ì¶œ ê°„ê²© (ì´ˆ)")
    parser.add_argument("--no-aggregate", action="store_true", help="ë‚ ì§œë³„ ì§‘ê³„ ì•ˆ í•¨")
    
    args = parser.parse_args()
    
    df_result = process_news_with_llm(
        input_path=args.input,
        output_path=args.output,
        provider=args.provider,
        model=args.model,
        api_key=args.api_key,
        title_column=args.title_column,
        date_column=args.date_column,
        stock_name=args.stock_name,
        delay=args.delay,
        aggregate=not args.no_aggregate
    )
    
    print("\n" + "=" * 60)
    print("ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
    print("=" * 60)
    print(df_result.head(10))

