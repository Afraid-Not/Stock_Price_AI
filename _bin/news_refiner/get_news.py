"""
í•œêµ­íˆ¬ìì¦ê¶Œ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ëª¨ë“ˆ
"""
import requests
import json
import pandas as pd
import time
import os
import sys
import argparse
from datetime import datetime, timedelta
from dotenv import load_dotenv

# ìƒìœ„ ë””ë ‰í† ë¦¬ì˜ s00_get_token ëª¨ë“ˆ import
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '_v6'))
from s00_get_token import get_access_token

load_dotenv()

# ì„¤ì •
APP_KEY = os.getenv("REAL_APP_KEY")
APP_SECRET = os.getenv("REAL_APP_SECRET")
BASE_URL = "https://openapi.koreainvestment.com:9443"

# TR_ID
TR_ID_NEWS_TITLE = "FHKST01011800"  # ì¢…í•© ì‹œí™©/ê³µì‹œ(ì œëª©)

# ì¬ì‹œë„ ì„¤ì •
MAX_RETRIES = 5
RETRY_DELAY = 2  # ì´ˆ

# ì»¬ëŸ¼ ë§¤í•‘ (í•œê¸€ëª…)
COLUMN_MAPPING = {
    'output1': 'ì‘ë‹µìƒì„¸',
    'cntt_usiq_srno': 'ë‚´ìš©_ì¡°íšŒìš©_ì¼ë ¨ë²ˆí˜¸',
    'news_ofer_entp_code': 'ë‰´ìŠ¤_ì œê³µ_ì—…ì²´_ì½”ë“œ',
    'data_dt': 'ì‘ì„±ì¼ì',
    'data_tm': 'ì‘ì„±ì‹œê°„',
    'hts_pbnt_titl_cntt': 'HTS_ê³µì‹œ_ì œëª©_ë‚´ìš©',
    'news_lrdv_code': 'ë‰´ìŠ¤_ëŒ€êµ¬ë¶„',
    'dorg': 'ìë£Œì›',
    'iscd1': 'ì¢…ëª©ì½”ë“œ1',
    'iscd2': 'ì¢…ëª©ì½”ë“œ2',
    'iscd3': 'ì¢…ëª©ì½”ë“œ3',
    'iscd4': 'ì¢…ëª©ì½”ë“œ4',
    'iscd5': 'ì¢…ëª©ì½”ë“œ5'
}


def get_news_title(
    token: str,
    fid_news_ofer_entp_code: str = "",  # ë‰´ìŠ¤ ì œê³µ ì—…ì²´ ì½”ë“œ (ê³µë°±: ì „ì²´)
    fid_cond_mrkt_cls_code: str = "",  # ì¡°ê±´ ì‹œì¥ êµ¬ë¶„ ì½”ë“œ (ê³µë°±: ì „ì²´)
    fid_input_iscd: str = "",  # ì…ë ¥ ì¢…ëª©ì½”ë“œ (ê³µë°±: ì „ì²´, ì¢…ëª©ì½”ë“œ: í•´ë‹¹ ì¢…ëª© ë‰´ìŠ¤)
    fid_titl_cntt: str = "",  # ì œëª© ë‚´ìš© (ê³µë°±: ì „ì²´, í‚¤ì›Œë“œ: ê²€ìƒ‰)
    fid_input_date_1: str = "",  # ì…ë ¥ ë‚ ì§œ (ê³µë°±: í˜„ì¬ê¸°ì¤€, YYYYMMDD í˜•ì‹)
    fid_input_hour_1: str = "",  # ì…ë ¥ ì‹œê°„ (ê³µë°±: í˜„ì¬ê¸°ì¤€, HHMMSS í˜•ì‹)
    fid_rank_sort_cls_code: str = "",  # ìˆœìœ„ ì •ë ¬ êµ¬ë¶„ ì½”ë“œ (ê³µë°±: ê¸°ë³¸)
    fid_input_srno: str = "",  # ì…ë ¥ ì¼ë ¨ë²ˆí˜¸ (ê³µë°±: ì²˜ìŒë¶€í„°)
    tr_cont: str = "",  # ì—°ì† ê±°ë˜ ì—¬ë¶€
    dataframe: pd.DataFrame = None,  # ëˆ„ì  ë°ì´í„°í”„ë ˆì„
    max_depth: int = 10  # ìµœëŒ€ í˜ì´ì§• ê¹Šì´
) -> pd.DataFrame:
    """
    ì¢…í•© ì‹œí™©/ê³µì‹œ(ì œëª©) APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë‰´ìŠ¤ ì œëª© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        token: ì•¡ì„¸ìŠ¤ í† í°
        fid_news_ofer_entp_code: ë‰´ìŠ¤ ì œê³µ ì—…ì²´ ì½”ë“œ
        fid_cond_mrkt_cls_code: ì¡°ê±´ ì‹œì¥ êµ¬ë¶„ ì½”ë“œ
        fid_input_iscd: ì…ë ¥ ì¢…ëª©ì½”ë“œ (ì˜ˆ: "005930" - ì‚¼ì„±ì „ì)
        fid_titl_cntt: ì œëª© ë‚´ìš© (í‚¤ì›Œë“œ ê²€ìƒ‰)
        fid_input_date_1: ì…ë ¥ ë‚ ì§œ (YYYYMMDD í˜•ì‹, ê³µë°±: í˜„ì¬ê¸°ì¤€)
        fid_input_hour_1: ì…ë ¥ ì‹œê°„ (HHMMSS í˜•ì‹, ê³µë°±: í˜„ì¬ê¸°ì¤€)
        fid_rank_sort_cls_code: ìˆœìœ„ ì •ë ¬ êµ¬ë¶„ ì½”ë“œ
        fid_input_srno: ì…ë ¥ ì¼ë ¨ë²ˆí˜¸
        tr_cont: ì—°ì† ê±°ë˜ ì—¬ë¶€
        dataframe: ëˆ„ì  ë°ì´í„°í”„ë ˆì„
        max_depth: ìµœëŒ€ í˜ì´ì§• ê¹Šì´
        
    Returns:
        DataFrame: ë‰´ìŠ¤ ì œëª© ë°ì´í„°
    """
    path = "/uapi/domestic-stock/v1/quotations/news-title"
    url = f"{BASE_URL}{path}"
    
    headers = {
        "Content-Type": "application/json",
        "authorization": f"Bearer {token}",
        "appKey": APP_KEY,
        "appSecret": APP_SECRET,
        "tr_id": TR_ID_NEWS_TITLE
    }
    
    params = {
        "FID_NEWS_OFER_ENTP_CODE": fid_news_ofer_entp_code,
        "FID_COND_MRKT_CLS_CODE": fid_cond_mrkt_cls_code,
        "FID_INPUT_ISCD": fid_input_iscd,
        "FID_TITL_CNTT": fid_titl_cntt,
        "FID_INPUT_DATE_1": fid_input_date_1,
        "FID_INPUT_HOUR_1": fid_input_hour_1,
        "FID_RANK_SORT_CLS_CODE": fid_rank_sort_cls_code,
        "FID_INPUT_SRNO": fid_input_srno,
    }
    
    # í˜ì´ì§• ê¹Šì´ ì²´í¬
    depth = len(str(dataframe).split('\n')) if dataframe is not None and not dataframe.empty else 0
    if depth >= max_depth:
        print(f"âš ï¸ ìµœëŒ€ í˜ì´ì§• ê¹Šì´({max_depth})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
        return dataframe if dataframe is not None else pd.DataFrame()
    
    # ì¬ì‹œë„ ë¡œì§
    for attempt in range(MAX_RETRIES):
        try:
            res = requests.get(url, headers=headers, params=params, timeout=30)
            
            if res.status_code == 200:
                data = res.json()
                
                if data.get('rt_cd') == '0':
                    # ì‘ë‹µ ë°ì´í„° ì²˜ë¦¬
                    output_data = data.get('output', [])
                    if not isinstance(output_data, list):
                        output_data = [output_data] if output_data else []
                    
                    if output_data:
                        current_data = pd.DataFrame(output_data)
                    else:
                        current_data = pd.DataFrame()
                    
                    # ë°ì´í„°í”„ë ˆì„ ë³‘í•©
                    if dataframe is not None and not dataframe.empty:
                        dataframe = pd.concat([dataframe, current_data], ignore_index=True)
                    else:
                        dataframe = current_data
                    
                    # ì—°ì† ê±°ë˜ ì—¬ë¶€ í™•ì¸ (í˜ì´ì§•)
                    tr_cont = data.get('tr_cd', '')
                    if tr_cont == "M" and not current_data.empty:
                        print(f"  ğŸ“„ ë‹¤ìŒ í˜ì´ì§€ ì¡°íšŒ ì¤‘... (ê¹Šì´: {depth + 1})")
                        time.sleep(1.0)  # API í˜¸ì¶œ ì œí•œ ê³ ë ¤
                        
                        # ë§ˆì§€ë§‰ ì¼ë ¨ë²ˆí˜¸ë¥¼ ë‹¤ìŒ ìš”ì²­ì— ì‚¬ìš©
                        last_srno = current_data['cntt_usiq_srno'].iloc[-1] if 'cntt_usiq_srno' in current_data.columns else ""
                        
                        return get_news_title(
                            token=token,
                            fid_news_ofer_entp_code=fid_news_ofer_entp_code,
                            fid_cond_mrkt_cls_code=fid_cond_mrkt_cls_code,
                            fid_input_iscd=fid_input_iscd,
                            fid_titl_cntt=fid_titl_cntt,
                            fid_input_date_1=fid_input_date_1,
                            fid_input_hour_1=fid_input_hour_1,
                            fid_rank_sort_cls_code=fid_rank_sort_cls_code,
                            fid_input_srno=last_srno,
                            tr_cont="N",
                            dataframe=dataframe,
                            max_depth=max_depth
                        )
                    else:
                        print(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(dataframe) if dataframe is not None else 0}ê±´")
                        return dataframe if dataframe is not None else pd.DataFrame()
                else:
                    error_msg = data.get('msg1', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                    print(f"âš ï¸ API ì˜¤ë¥˜ ì½”ë“œ: {data.get('rt_cd')}, ë©”ì‹œì§€: {error_msg}")
                    return dataframe if dataframe is not None else pd.DataFrame()
            else:
                print(f"âš ï¸ HTTP ìƒíƒœ ì½”ë“œ: {res.status_code}")
                
        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout,
                requests.exceptions.RequestException) as e:
            if attempt < MAX_RETRIES - 1:
                wait_time = RETRY_DELAY * (attempt + 1)
                print(f"ğŸ”„ ì—°ê²° ì˜¤ë¥˜ ë°œìƒ, {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{MAX_RETRIES})...")
                time.sleep(wait_time)
            else:
                print(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {e}")
                return dataframe if dataframe is not None else pd.DataFrame()
    
    return dataframe if dataframe is not None else pd.DataFrame()


def generate_file_path(output_path: str, stock_code: str = "", start_date: str = "", end_date: str = "") -> str:
    """
    ì¶œë ¥ ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë””ë ‰í† ë¦¬ë§Œ ì…ë ¥ë˜ë©´ ìë™ìœ¼ë¡œ íŒŒì¼ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        output_path: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì¶œë ¥ ê²½ë¡œ
        stock_code: ì¢…ëª©ì½”ë“œ
        start_date: ì‹œì‘ ë‚ ì§œ
        end_date: ì¢…ë£Œ ë‚ ì§œ
        
    Returns:
        str: ì™„ì „í•œ íŒŒì¼ ê²½ë¡œ
    """
    # ì´ë¯¸ íŒŒì¼ëª…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (.csv, .xlsx ë“± í™•ì¥ì í™•ì¸)
    if output_path.endswith(('.csv', '.xlsx', '.xls', '.json', '.parquet')):
        return output_path
    
    # ë””ë ‰í† ë¦¬ ê²½ë¡œì¸ ê²½ìš°
    # ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
    if not os.path.exists(output_path):
        os.makedirs(output_path, exist_ok=True)
    
    # íŒŒì¼ëª… ìƒì„±
    filename_parts = []
    
    if stock_code:
        filename_parts.append(f"news_{stock_code}")
    else:
        filename_parts.append("news_total")
    
    if start_date and end_date:
        if start_date == end_date:
            filename_parts.append(start_date)
        else:
            filename_parts.append(f"{start_date}_{end_date}")
    elif start_date:
        filename_parts.append(start_date)
    else:
        filename_parts.append(datetime.now().strftime("%Y%m%d"))
    
    filename = "_".join(filename_parts) + ".csv"
    
    return os.path.join(output_path, filename)


def collect_news(
    stock_code: str = "",
    keyword: str = "",
    start_date: str = "",
    end_date: str = "",
    save_path: str = None
) -> pd.DataFrame:
    """
    ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    
    Args:
        stock_code: ì¢…ëª©ì½”ë“œ (ì˜ˆ: "005930" - ì‚¼ì„±ì „ì, ê³µë°±: ì „ì²´)
        keyword: ì œëª© í‚¤ì›Œë“œ ê²€ìƒ‰ (ê³µë°±: ì „ì²´)
        start_date: ì‹œì‘ ë‚ ì§œ (YYYYMMDD í˜•ì‹, ê³µë°±: í˜„ì¬ê¸°ì¤€)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD í˜•ì‹, ê³µë°±: í˜„ì¬ê¸°ì¤€)
        save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        
    Returns:
        DataFrame: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°
        
    Note:
        - start_dateì™€ end_dateê°€ ëª¨ë‘ ì§€ì •ë˜ë©´ ê¸°ê°„ ë²”ìœ„ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤
        - APIëŠ” ë‚ ì§œë³„ë¡œ ì¡°íšŒí•˜ë¯€ë¡œ, ê¸°ê°„ ë²”ìœ„ëŠ” ê° ë‚ ì§œë¥¼ ìˆœíšŒí•˜ë©° ìˆ˜ì§‘í•©ë‹ˆë‹¤
        - ì¼ë°˜ì ìœ¼ë¡œ ìµœê·¼ 1ë…„ ì •ë„ì˜ ë°ì´í„°ë¥¼ ì œê³µí•˜ì§€ë§Œ, ì •í™•í•œ ê¸°ê°„ ì œí•œì€ APIì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    """
    # í† í° ë°œê¸‰
    print("ğŸ”‘ í† í° ë°œê¸‰ ì¤‘...")
    token = get_access_token()
    if not token:
        print("âŒ í† í° ë°œê¸‰ ì‹¤íŒ¨")
        return pd.DataFrame()
    print("âœ… í† í° ë°œê¸‰ ì™„ë£Œ")
    
    # ë‚ ì§œ ì„¤ì •
    if not start_date:
        # ê¸°ë³¸ê°’: ì˜¤ëŠ˜ ë‚ ì§œ
        start_date = datetime.now().strftime("%Y%m%d")
    
    # ê¸°ê°„ ë²”ìœ„ ì¡°íšŒì¸ì§€ í™•ì¸
    if end_date and start_date != end_date:
        return collect_news_by_period(
            token=token,
            stock_code=stock_code,
            keyword=keyword,
            start_date=start_date,
            end_date=end_date,
            save_path=save_path
        )
    
    print(f"\nğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    print(f"  ì¢…ëª©ì½”ë“œ: {stock_code if stock_code else 'ì „ì²´'}")
    print(f"  í‚¤ì›Œë“œ: {keyword if keyword else 'ì „ì²´'}")
    print(f"  ì¡°íšŒ ë‚ ì§œ: {start_date}")
    
    # ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
    df_news = get_news_title(
        token=token,
        fid_input_iscd=stock_code,
        fid_titl_cntt=keyword,
        fid_input_date_1=start_date,
        fid_input_hour_1="",  # ì‹œê°„ì€ ê³µë°±ìœ¼ë¡œ ì „ì²´ ì¡°íšŒ
        max_depth=20  # ìµœëŒ€ 20í˜ì´ì§€ê¹Œì§€ ì¡°íšŒ
    )
    
    if df_news.empty:
        print("âš ï¸ ì¡°íšŒëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # ì»¬ëŸ¼ëª… í•œê¸€ ë³€í™˜
    df_news = df_news.rename(columns=COLUMN_MAPPING)
    
    print(f"\nâœ… ì´ {len(df_news)}ê±´ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
    
    # ì €ì¥
    if save_path:
        df_news.to_csv(save_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_path}")
    
    return df_news


def collect_news_by_period(
    token: str,
    stock_code: str = "",
    keyword: str = "",
    start_date: str = "",
    end_date: str = "",
    save_path: str = None
) -> pd.DataFrame:
    """
    ê¸°ê°„ ë²”ìœ„ë¡œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        token: ì•¡ì„¸ìŠ¤ í† í°
        stock_code: ì¢…ëª©ì½”ë“œ (ì˜ˆ: "005930" - ì‚¼ì„±ì „ì, ê³µë°±: ì „ì²´)
        keyword: ì œëª© í‚¤ì›Œë“œ ê²€ìƒ‰ (ê³µë°±: ì „ì²´)
        start_date: ì‹œì‘ ë‚ ì§œ (YYYYMMDD í˜•ì‹)
        end_date: ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD í˜•ì‹)
        save_path: ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        
    Returns:
        DataFrame: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°
        
    Note:
        - APIëŠ” ë‚ ì§œë³„ë¡œ ì¡°íšŒí•˜ë¯€ë¡œ, ê° ë‚ ì§œë¥¼ ìˆœíšŒí•˜ë©° ìˆ˜ì§‘í•©ë‹ˆë‹¤
        - ì£¼ë§/ê³µíœ´ì¼ ë“± ì¥ì´ ì—´ë¦¬ì§€ ì•Šì€ ë‚ ì€ ë°ì´í„°ê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
        - ì¼ë°˜ì ìœ¼ë¡œ ìµœê·¼ 1ë…„ ì •ë„ì˜ ë°ì´í„°ë¥¼ ì œê³µí•˜ì§€ë§Œ, ì •í™•í•œ ê¸°ê°„ ì œí•œì€ APIì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    """
    print(f"\nğŸ“° ê¸°ê°„ ë²”ìœ„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
    print(f"  ì¢…ëª©ì½”ë“œ: {stock_code if stock_code else 'ì „ì²´'}")
    print(f"  í‚¤ì›Œë“œ: {keyword if keyword else 'ì „ì²´'}")
    print(f"  ê¸°ê°„: {start_date} ~ {end_date}")
    
    # ë‚ ì§œ ë²”ìœ„ ìƒì„±
    start_dt = datetime.strptime(start_date, "%Y%m%d")
    end_dt = datetime.strptime(end_date, "%Y%m%d")
    
    if start_dt > end_dt:
        print("âŒ ì‹œì‘ ë‚ ì§œê°€ ì¢…ë£Œ ë‚ ì§œë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
    
    # ë‚ ì§œë³„ë¡œ ìˆ˜ì§‘
    df_all_news = []
    current_dt = start_dt
    total_days = (end_dt - start_dt).days + 1
    
    print(f"  ì´ {total_days}ì¼ê°„ì˜ ë°ì´í„° ìˆ˜ì§‘ ì˜ˆì •...\n")
    
    while current_dt <= end_dt:
        date_str = current_dt.strftime("%Y%m%d")
        day_num = (current_dt - start_dt).days + 1
        
        print(f"  [{day_num}/{total_days}] {date_str} ìˆ˜ì§‘ ì¤‘...", end=" ")
        
        df_day = get_news_title(
            token=token,
            fid_input_iscd=stock_code,
            fid_titl_cntt=keyword,
            fid_input_date_1=date_str,
            fid_input_hour_1="",
            max_depth=20
        )
        
        if not df_day.empty:
            df_all_news.append(df_day)
            print(f"âœ… {len(df_day)}ê±´")
        else:
            print("âš ï¸ ë°ì´í„° ì—†ìŒ")
        
        # ë‹¤ìŒ ë‚ ì§œë¡œ ì´ë™
        current_dt += timedelta(days=1)
        
        # API í˜¸ì¶œ ì œí•œ ê³ ë ¤ (ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ ë°©ì§€)
        time.sleep(0.5)
    
    # ëª¨ë“  ë°ì´í„° ë³‘í•©
    if df_all_news:
        df_news = pd.concat(df_all_news, ignore_index=True)
        # ì»¬ëŸ¼ëª… í•œê¸€ ë³€í™˜
        df_news = df_news.rename(columns=COLUMN_MAPPING)
        
        # ì¤‘ë³µ ì œê±° (í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ë°©ì§€)
        if 'ë‚´ìš©_ì¡°íšŒìš©_ì¼ë ¨ë²ˆí˜¸' in df_news.columns:
            df_news = df_news.drop_duplicates(subset=['ë‚´ìš©_ì¡°íšŒìš©_ì¼ë ¨ë²ˆí˜¸'], keep='first')
        
        # ë‚ ì§œìˆœ ì •ë ¬
        if 'ì‘ì„±ì¼ì' in df_news.columns and 'ì‘ì„±ì‹œê°„' in df_news.columns:
            df_news = df_news.sort_values(['ì‘ì„±ì¼ì', 'ì‘ì„±ì‹œê°„'], ascending=[False, False])
        
        print(f"\nâœ… ì´ {len(df_news)}ê±´ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ì €ì¥
        if save_path:
            df_news.to_csv(save_path, index=False, encoding='utf-8-sig')
            print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {save_path}")
        
        return df_news
    else:
        print("\nâš ï¸ ì¡°íšŒëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="í•œêµ­íˆ¬ìì¦ê¶Œ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ì˜¤ëŠ˜ì˜ ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘
  python get_news.py -o news_total.csv
  
  # íŠ¹ì • ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ (ì˜¤ëŠ˜)
  python get_news.py -c 005930 -o news_005930.csv
  
  # í‚¤ì›Œë“œ ê²€ìƒ‰
  python get_news.py -k "ë°°ë‹¹" -o news_dividend.csv
  
  # ê¸°ê°„ ë²”ìœ„ ë‰´ìŠ¤ ìˆ˜ì§‘
  python get_news.py -c 005930 -s 20260122 -e 20260129 -o news_period.csv
  
  # ìµœê·¼ 7ì¼ê°„ ë‰´ìŠ¤ ìˆ˜ì§‘
  python get_news.py -c 005930 --days 7 -o news_7days.csv
        """
    )
    
    parser.add_argument(
        "-o", "--output",
        type=str,
        required=True,
        help="ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ì˜ˆ: news_total.csv ë˜ëŠ” news_refiner/news_005930.csv)"
    )
    
    parser.add_argument(
        "-c", "--code",
        type=str,
        default="",
        help="ì¢…ëª©ì½”ë“œ (ì˜ˆ: 005930 - ì‚¼ì„±ì „ì, ê¸°ë³¸ê°’: ì „ì²´)"
    )
    
    parser.add_argument(
        "-k", "--keyword",
        type=str,
        default="",
        help="ì œëª© í‚¤ì›Œë“œ ê²€ìƒ‰ (ê¸°ë³¸ê°’: ì „ì²´)"
    )
    
    parser.add_argument(
        "-s", "--start-date",
        type=str,
        default="",
        help="ì‹œì‘ ë‚ ì§œ (YYYYMMDD í˜•ì‹, ê¸°ë³¸ê°’: ì˜¤ëŠ˜)"
    )
    
    parser.add_argument(
        "-e", "--end-date",
        type=str,
        default="",
        help="ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD í˜•ì‹, ê¸°ë³¸ê°’: ì‹œì‘ì¼ê³¼ ë™ì¼)"
    )
    
    parser.add_argument(
        "--days",
        type=int,
        default=None,
        help="ìµœê·¼ Nì¼ê°„ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (ì˜ˆ: --days 7)"
    )
    
    args = parser.parse_args()
    
    # days ì˜µì…˜ì´ ìˆìœ¼ë©´ start_dateì™€ end_date ìë™ ì„¤ì •
    if args.days:
        end_date = datetime.now().strftime("%Y%m%d")
        start_date = (datetime.now() - timedelta(days=args.days - 1)).strftime("%Y%m%d")
    else:
        start_date = args.start_date if args.start_date else ""
        end_date = args.end_date if args.end_date else ""
    
    # ì¶œë ¥ ê²½ë¡œ ìƒì„± (ë””ë ‰í† ë¦¬ë§Œ ì…ë ¥ëœ ê²½ìš° ìë™ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±)
    save_path = generate_file_path(
        output_path=args.output,
        stock_code=args.code,
        start_date=start_date,
        end_date=end_date
    )
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰
    print("=" * 60)
    print("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    print("=" * 60)
    print(f"ì €ì¥ ê²½ë¡œ: {save_path}")
    
    df = collect_news(
        stock_code=args.code,
        keyword=args.keyword,
        start_date=start_date,
        end_date=end_date,
        save_path=save_path
    )
    
    if not df.empty:
        print(f"\nìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì •ë³´:")
        print(f"  ì´ ê±´ìˆ˜: {len(df)}ê±´")
        if 'ì‘ì„±ì¼ì' in df.columns:
            print(f"  ë‚ ì§œ ë²”ìœ„: {df['ì‘ì„±ì¼ì'].min()} ~ {df['ì‘ì„±ì¼ì'].max()}")
        print(f"\nì²˜ìŒ 5ê±´ ë¯¸ë¦¬ë³´ê¸°:")
        print(df.head())
    else:
        print("\nâš ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

