import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit
import joblib
import os

from datetime import datetime
now_str = datetime.now().strftime("%Y%m%d_%H%M%S")

# [í™˜ê²½ ì„¤ì •]
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
STOCK_PATH = "D:/stock/_data/manual_fetch/preprocessed_005930_20100101_20251231.csv"
NEWS_PATH = "D:/stock/_data/refined_news/daily_sentiment_score.csv"
STAGE1_MODEL_PATH = "d:/stock/lstm_model/stage1_best_model.pth"
STAGE1_SCALER_X = "d:/stock/lstm_model/stage1_scaler_x.pkl"
STAGE2_MODEL_SAVE = f"d:/stock/lstm_model/stage2_final_model_{now_str}.pth"

# 1. ë°ì´í„° ë³‘í•© ë° 2ë‹¨ê³„ êµ¬ê°„ í•„í„°ë§ (2022ë…„ ì´í›„)
stock_df = pd.read_csv(STOCK_PATH)
news_df = pd.read_csv(NEWS_PATH)

stock_df['ë‚ ì§œ'] = pd.to_datetime(stock_df['ë‚ ì§œ'])
news_df['ë‚ ì§œ'] = pd.to_datetime(news_df['ë‚ ì§œ'])

# 2022ë…„ ì´í›„ ë°ì´í„°ë§Œ ì¶”ì¶œí•˜ì—¬ ë³‘í•©
stage2_df = pd.merge(stock_df[stock_df['ë‚ ì§œ'] >= '2022-01-01'], news_df, on='ë‚ ì§œ', how='left')
stage2_df['news_sentiment'] = stage2_df['news_sentiment'].fillna(0).reset_index(drop=True)
stage2_df = stage2_df.reset_index(drop=True)

feature_cols = [c for c in stock_df.columns if c not in ['ë‚ ì§œ', 'target']]
target_col = 'target'

# 2. 1ë‹¨ê³„ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ë° ì ìš©
scaler_x = joblib.load(STAGE1_SCALER_X)
# ë‰´ìŠ¤ ì ìˆ˜ëŠ” ì´ë¯¸ -1 ~ 1 ì‚¬ì´ì´ë¯€ë¡œ ìŠ¤ì¼€ì¼ë§ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
X_tech_scaled = scaler_x.transform(stage2_df[feature_cols])
X_news = stage2_df[['news_sentiment']].values
y_true = stage2_df[[target_col]].values

# 3. ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜ (ê¸°ìˆ ì  ì§€í‘œì™€ ë‰´ìŠ¤ë¥¼ í•¨ê»˜ ë¬¶ìŒ)
def create_fusion_sequences(tech_data, news_data, target_data, window_size=20):
    seq_x_tech, seq_x_news, seq_y = [], [], []
    for i in range(len(tech_data) - window_size):
        seq_x_tech.append(tech_data[i : i + window_size])
        # ë‰´ìŠ¤ëŠ” 'ë‹¹ì¼'ì˜ ë‰´ìŠ¤ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ íƒ€ê²Ÿê³¼ ê°™ì€ ì‹œì ì˜ ì ìˆ˜ ì‚¬ìš©
        seq_x_news.append(news_data[i + window_size])
        seq_y.append(target_data[i + window_size])
    return torch.FloatTensor(np.array(seq_x_tech)), torch.FloatTensor(np.array(seq_x_news)), torch.FloatTensor(np.array(seq_y))

# 4. ë‰´ìŠ¤ ê²°í•©í˜• ëª¨ë¸ ì •ì˜
class StockNewsFusionModel(nn.Module):
    def __init__(self, tech_dim, news_dim=1, hidden_dim=64):
        super(StockNewsFusionModel, self).__init__()
        self.lstm = nn.LSTM(tech_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.2)
        # LSTM ì¶œë ¥(hidden_dim) + ë‰´ìŠ¤ ì ìˆ˜(news_dim)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” ìµœì¢… ë ˆì´ì–´
        self.fc_final = nn.Linear(hidden_dim + news_dim, 1)

    def forward(self, x_tech, x_news):
        lstm_out, _ = self.lstm(x_tech)
        last_tech_feat = lstm_out[:, -1, :] # ì°¨íŠ¸ì˜ íŠ¹ì§• ì¶”ì¶œ
        
        # ì£¼ê°€ íŠ¹ì§•ê³¼ ë‰´ìŠ¤ ì‹¬ë¦¬ë¥¼ ê²°í•©
        combined = torch.cat((last_tech_feat, x_news), dim=1)
        return self.fc_final(combined)

# 5. TimeSeriesSplit ê¸°ë°˜ 2ë‹¨ê³„ í•™ìŠµ
tscv = TimeSeriesSplit(n_splits=3) # 2022-2025 ë°ì´í„°ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì ìœ¼ë¯€ë¡œ 3 splits ê¶Œì¥
WINDOW_SIZE = 20
best_stage2_loss = float('inf')



print(f"ğŸš€ ë‰´ìŠ¤ ê²°í•© 2ë‹¨ê³„ í•™ìŠµ ì‹œì‘ (êµ¬ê°„: {stage2_df['ë‚ ì§œ'].min().date()} ~ )")

for train_idx, val_idx in tscv.split(stage2_df):
    # ë°ì´í„° ë¶„í• 
    train_tech, val_tech = X_tech_scaled[train_idx], X_tech_scaled[val_idx]
    train_news, val_news = X_news[train_idx], X_news[val_idx]
    train_y, val_y = y_true[train_idx], y_true[val_idx]
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    X_train_tech, X_train_news, y_train = create_fusion_sequences(train_tech, train_news, train_y, WINDOW_SIZE)
    X_val_tech, X_val_news, y_val = create_fusion_sequences(val_tech, val_news, val_y, WINDOW_SIZE)
    
    # ëª¨ë¸ ì´ˆê¸°í™” ë° ê°€ì¤‘ì¹˜ ì´ì‹
    model = StockNewsFusionModel(tech_dim=len(feature_cols)).to(DEVICE)
    stage1_state = torch.load(STAGE1_MODEL_PATH)
    # 1ë‹¨ê³„ì˜ LSTM ë¶€ë¶„ ê°€ì¤‘ì¹˜ë§Œ ë§¤ì¹­í•˜ì—¬ ë¡œë“œ
    model.lstm.load_state_dict({k.replace('lstm.', ''): v for k, v in stage1_state.items() if 'lstm' in k})
    
    # ì „ëµ: ì²˜ìŒ ëª‡ ì—í¬í¬ëŠ” LSTMì„ ê³ ì •(Freeze)í•˜ê³  fc_finalë§Œ í•™ìŠµí•˜ì—¬ ë‰´ìŠ¤ íš¨ê³¼ë¥¼ ë¨¼ì € ë°˜ì˜
    optimizer = optim.Adam(model.parameters(), lr=0.0005)
    criterion = nn.MSELoss()
    
    for epoch in range(30): # ë¯¸ì„¸ ì¡°ì •ì´ë¯€ë¡œ ì—í¬í¬ëŠ” ì ê²Œ ì„¤ì •
        model.train()
        optimizer.zero_grad()
        output = model(X_train_tech.to(DEVICE), X_train_news.to(DEVICE))
        loss = criterion(output, y_train.to(DEVICE))
        loss.backward()
        optimizer.step()
        
        # Validation
        model.eval()
        with torch.no_grad():
            val_output = model(X_val_tech.to(DEVICE), X_val_news.to(DEVICE))
            v_loss = criterion(val_output, y_val.to(DEVICE)).item()
            
            if v_loss < best_stage2_loss:
                best_stage2_loss = v_loss
                torch.save(model.state_dict(), STAGE2_MODEL_SAVE)

print(f"âœ¨ 2ë‹¨ê³„ ìµœì¢… ì™„ë£Œ! ìµœì  ë‰´ìŠ¤ ê²°í•© Loss: {best_stage2_loss:.6f}")