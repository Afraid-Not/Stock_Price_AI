"""
XGBoost + LightGBM + CatBoost ì•™ìƒë¸” í•™ìŠµ
TimeSeriesSplit ê¸°ë°˜ ê²€ì¦
"""
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
import joblib
import os
from datetime import datetime


class StockEnsembleTrainer:
    def __init__(self, data_path, n_splits=5, lag_days=[1, 2, 3, 5, 10], 
                 target_threshold=0.01):
        """
        Args:
            data_path: ë°ì´í„° ê²½ë¡œ
            n_splits: K-Fold ìˆ˜
            lag_days: Lag í”¼ì²˜ ìƒì„±í•  ì¼ìˆ˜
            target_threshold: íƒ€ê²Ÿ ì„ê³„ê°’ (0.01 = 1%)
                - Noneì´ë©´ ê¸°ì¡´ íƒ€ê²Ÿ ì‚¬ìš© (0ë³´ë‹¤ í¬ë©´ 1)
                - 0.01ì´ë©´ 1% ì´ìƒ ìƒìŠ¹=1, 1% ì´ìƒ í•˜ë½=0, ë‚˜ë¨¸ì§€ ì œì™¸
        """
        self.data_path = data_path
        self.n_splits = n_splits
        self.lag_days = lag_days
        self.target_threshold = target_threshold
        self.models = {}
        self.label_encoder = LabelEncoder()
        
        # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        self.model_dir = 'models'
        os.makedirs(self.model_dir, exist_ok=True)
        
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
        df = pd.read_csv(self.data_path)
        
        # ë‚ ì§œ ì •ë ¬
        df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])
        df = df.sort_values(['stock_code', 'ë‚ ì§œ']).reset_index(drop=True)
        
        print(f"   ì›ë³¸ ë°ì´í„°: {len(df):,}ê±´")
        print(f"   ê¸°ê°„: {df['ë‚ ì§œ'].min().date()} ~ {df['ë‚ ì§œ'].max().date()}")
        print(f"   ì¢…ëª© ìˆ˜: {df['stock_code'].nunique()}ê°œ")
        
        # íƒ€ê²Ÿ ì¬ì •ì˜
        if self.target_threshold is not None:
            df = self.redefine_target(df)
        
        # Lag í”¼ì²˜ ì¶”ê°€
        df = self.add_lag_features(df)
        
        print(f"   Lag í”¼ì²˜ ì¶”ê°€ í›„: {len(df):,}ê±´")
        print(f"   í´ë˜ìŠ¤ ë¶„í¬: 0={len(df[df['target']==0]):,}, 1={len(df[df['target']==1]):,}")
        
        return df
    
    def redefine_target(self, df):
        """íƒ€ê²Ÿ ì¬ì •ì˜: ì„ê³„ê°’ ì´ìƒ ìƒìŠ¹/í•˜ë½ë§Œ ì‚¬ìš©
        
        next_rtn = (ë‚´ì¼ ì¢…ê°€ - ì˜¤ëŠ˜ ì¢…ê°€) / ì˜¤ëŠ˜ ì¢…ê°€
        â†’ ì˜¤ëŠ˜ ì¢…ê°€ ê¸°ì¤€ ë‚´ì¼ ì¢…ê°€ ìˆ˜ìµë¥ 
        """
        print(f"\nğŸ¯ íƒ€ê²Ÿ ì¬ì •ì˜ (ì¢…ê°€â†’ë‹¤ìŒë‚  ì¢…ê°€, ì„ê³„ê°’: Â±{self.target_threshold*100:.1f}%)")
        
        # next_rtnì´ ì´ë¯¸ ì „ì²˜ë¦¬ ë°ì´í„°ì— í¬í•¨ë˜ì–´ ìˆìŒ
        if 'next_rtn' not in df.columns:
            raise ValueError("next_rtn ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
        
        # NaN ì œê±° (ë§ˆì§€ë§‰ í–‰)
        df = df.dropna(subset=['next_rtn'])
        
        before_filter = len(df)
        
        # íƒ€ê²Ÿ ì¬ì •ì˜
        # ì„ê³„ê°’ ì´ìƒ ìƒìŠ¹ = 1 (ì‹œê°€ ëŒ€ë¹„ ì¢…ê°€ +1% ì´ìƒ)
        # ì„ê³„ê°’ ì´ìƒ í•˜ë½ = 0 (ì‹œê°€ ëŒ€ë¹„ ì¢…ê°€ -1% ì´í•˜)
        # ê·¸ ì‚¬ì´ = ì œì™¸ (ë…¸ì´ì¦ˆ)
        df_up = df[df['next_rtn'] >= self.target_threshold].copy()
        df_up['target'] = 1
        
        df_down = df[df['next_rtn'] <= -self.target_threshold].copy()
        df_down['target'] = 0
        
        df_filtered = pd.concat([df_up, df_down], ignore_index=True)
        df_filtered = df_filtered.sort_values(['stock_code', 'ë‚ ì§œ']).reset_index(drop=True)
        
        # next_rtn ì»¬ëŸ¼ ì œê±° (í”¼ì²˜ë¡œ ì‚¬ìš©í•˜ë©´ ë°ì´í„° ëˆ„ìˆ˜!)
        df_filtered = df_filtered.drop(columns=['next_rtn'])
        
        after_filter = len(df_filtered)
        removed = before_filter - after_filter
        
        print(f"   ìƒìŠ¹ (ì¢…ê°€â†’ì¢…ê°€ â‰¥+{self.target_threshold*100:.1f}%): {len(df_up):,}ê±´")
        print(f"   í•˜ë½ (ì¢…ê°€â†’ì¢…ê°€ â‰¤-{self.target_threshold*100:.1f}%): {len(df_down):,}ê±´")
        print(f"   ì œì™¸ (ë…¸ì´ì¦ˆ êµ¬ê°„): {removed:,}ê±´ ({removed/before_filter*100:.1f}%)")
        print(f"   í•„í„°ë§ í›„: {after_filter:,}ê±´")
        
        return df_filtered
    
    def add_lag_features(self, df):
        """Lag í”¼ì²˜ ì¶”ê°€ (ì¢…ëª©ë³„ë¡œ)"""
        print(f"ğŸ“Š Lag í”¼ì²˜ ìƒì„± ì¤‘... (lag_days: {self.lag_days})")
        
        # Lag í”¼ì²˜ë¥¼ ë§Œë“¤ ì»¬ëŸ¼ë“¤ (ì¤‘ìš”í•œ í”¼ì²˜ë“¤)
        lag_cols = [
            'open_gap', 'high_ratio', 'low_ratio', 'volatility',
            'gap_ma5', 'gap_ma20', 'gap_ma60',
            'ê°œì¸_ì²´ê²°ê°•ë„', 'ì™¸êµ­ì¸_ì²´ê²°ê°•ë„', 'ê¸°ê´€ê³„_ì²´ê²°ê°•ë„',
            'vol_ratio', 'vol_ma5_ratio', 'rsi',
            'macd_ratio', 'macd_diff_ratio',
            'bb_upper_ratio', 'bb_lower_ratio'
        ]
        
        # ì¢…ëª©ë³„ë¡œ Lag í”¼ì²˜ ìƒì„±
        lag_dfs = []
        
        for stock_code in df['stock_code'].unique():
            stock_df = df[df['stock_code'] == stock_code].copy()
            stock_df = stock_df.sort_values('ë‚ ì§œ')
            
            # ê° Lagì— ëŒ€í•´ í”¼ì²˜ ìƒì„±
            for lag in self.lag_days:
                for col in lag_cols:
                    if col in stock_df.columns:
                        stock_df[f'{col}_lag{lag}'] = stock_df[col].shift(lag)
            
            # ë³€í™”ìœ¨ í”¼ì²˜ ì¶”ê°€ (1ì¼ ì „ ëŒ€ë¹„)
            for col in ['gap_ma5', 'rsi', 'ì™¸êµ­ì¸_ì²´ê²°ê°•ë„', 'ê¸°ê´€ê³„_ì²´ê²°ê°•ë„']:
                if col in stock_df.columns:
                    stock_df[f'{col}_change'] = stock_df[col] - stock_df[col].shift(1)
            
            lag_dfs.append(stock_df)
        
        df_with_lag = pd.concat(lag_dfs, ignore_index=True)
        
        # NaN ì œê±° (Lagë¡œ ì¸í•´ ì•ë¶€ë¶„ì— NaN ìƒê¹€)
        before_drop = len(df_with_lag)
        df_with_lag = df_with_lag.dropna().reset_index(drop=True)
        after_drop = len(df_with_lag)
        
        print(f"   ìƒì„±ëœ Lag í”¼ì²˜ ìˆ˜: {len([c for c in df_with_lag.columns if 'lag' in c or 'change' in c])}ê°œ")
        print(f"   NaN ì œê±°: {before_drop - after_drop:,}ê±´ ì œê±°ë¨")
        
        return df_with_lag
    
    def prepare_features(self, df):
        """í”¼ì²˜ ì¤€ë¹„"""
        # ì œì™¸í•  ì»¬ëŸ¼
        exclude_cols = ['ë‚ ì§œ', 'target', 'stock_code', 'stock_name']
        
        # í”¼ì²˜ ì»¬ëŸ¼
        feature_cols = [c for c in df.columns if c not in exclude_cols]
        
        X = df[feature_cols].copy()
        y = df['target'].values
        dates = df['ë‚ ì§œ'].values
        stock_codes = df['stock_code'].values
        
        # ì¢…ëª© ì½”ë“œë¥¼ ìˆ«ìë¡œ ì¸ì½”ë”© (í”¼ì²˜ë¡œ ì‚¬ìš©)
        stock_code_encoded = self.label_encoder.fit_transform(stock_codes)
        X['stock_code_encoded'] = stock_code_encoded
        
        print(f"\nğŸ“Š í”¼ì²˜ ì •ë³´:")
        print(f"   í”¼ì²˜ ìˆ˜: {len(X.columns)}ê°œ")
        print(f"   í”¼ì²˜ ëª©ë¡: {list(X.columns)}")
        
        return X, y, dates, stock_codes
    
    
    def train_xgboost(self, X_train, y_train, X_val, y_val):
        """XGBoost í•™ìŠµ"""
        print("\nğŸ”µ XGBoost í•™ìŠµ ì¤‘...")
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        scale_pos = len(y_train[y_train==0]) / len(y_train[y_train==1])
        
        model = xgb.XGBClassifier(
            n_estimators=1000,
            max_depth=8,
            learning_rate=0.01,
            subsample=0.8,
            colsample_bytree=0.8,
            min_child_weight=5,
            reg_alpha=0.1,
            reg_lambda=1.0,
            scale_pos_weight=scale_pos,
            random_state=42,
            n_jobs=-1,
            eval_metric='auc',
            early_stopping_rounds=100
        )
        
        model.fit(
            X_train, y_train,
            eval_set=[(X_train, y_train), (X_val, y_val)],
            verbose=100
        )
        
        print(f"   Best iteration: {model.best_iteration}")
        self.models['xgboost'] = model
        return model
    
    def train_lightgbm(self, X_train, y_train, X_val, y_val):
        """LightGBM í•™ìŠµ"""
        print("\nğŸŸ¢ LightGBM í•™ìŠµ ì¤‘...")
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        scale_pos = len(y_train[y_train==0]) / len(y_train[y_train==1])
        
        model = lgb.LGBMClassifier(
            n_estimators=1000,
            max_depth=8,
            learning_rate=0.01,
            subsample=0.8,
            colsample_bytree=0.8,
            min_child_samples=30,
            reg_alpha=0.1,
            reg_lambda=1.0,
            scale_pos_weight=scale_pos,
            random_state=42,
            n_jobs=-1,
            verbose=100
        )
        
        model.fit(
            X_train, y_train,
            eval_set=[(X_train, y_train), (X_val, y_val)],
            eval_metric='auc',
            callbacks=[lgb.early_stopping(100, verbose=True)]
        )
        
        print(f"   Best iteration: {model.best_iteration_}")
        self.models['lightgbm'] = model
        return model
    
    def train_catboost(self, X_train, y_train, X_val, y_val):
        """CatBoost í•™ìŠµ"""
        print("\nğŸŸ¡ CatBoost í•™ìŠµ ì¤‘...")
        
        # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        scale_pos = len(y_train[y_train==0]) / len(y_train[y_train==1])
        
        model = CatBoostClassifier(
            iterations=1000,
            depth=8,
            learning_rate=0.01,
            l2_leaf_reg=3,
            scale_pos_weight=scale_pos,
            random_seed=42,
            verbose=100,
            early_stopping_rounds=100,
            eval_metric='AUC'
        )
        
        model.fit(
            X_train, y_train,
            eval_set=(X_val, y_val),
            verbose=100
        )
        
        print(f"   Best iteration: {model.best_iteration_}")
        self.models['catboost'] = model
        return model
    
    def ensemble_predict(self, X, method='soft'):
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        predictions = {}
        probas = {}
        
        for name, model in self.models.items():
            pred = model.predict(X)
            proba = model.predict_proba(X)[:, 1]
            predictions[name] = pred
            probas[name] = proba
        
        if method == 'soft':
            # Soft Voting: í™•ë¥  í‰ê· 
            avg_proba = np.mean([probas[name] for name in self.models], axis=0)
            ensemble_pred = (avg_proba >= 0.5).astype(int)
            ensemble_proba = avg_proba
        else:
            # Hard Voting: ë‹¤ìˆ˜ê²°
            all_preds = np.array([predictions[name] for name in self.models])
            ensemble_pred = (np.mean(all_preds, axis=0) >= 0.5).astype(int)
            ensemble_proba = np.mean([probas[name] for name in self.models], axis=0)
        
        return ensemble_pred, ensemble_proba, predictions, probas
    
    def evaluate(self, y_true, y_pred, y_proba, model_name="Model"):
        """ì„±ëŠ¥ í‰ê°€"""
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1': f1_score(y_true, y_pred, zero_division=0),
            'auc': roc_auc_score(y_true, y_proba)
        }
        
        print(f"\nğŸ“ˆ {model_name} ì„±ëŠ¥:")
        print(f"   Accuracy:  {metrics['accuracy']:.4f}")
        print(f"   Precision: {metrics['precision']:.4f}")
        print(f"   Recall:    {metrics['recall']:.4f}")
        print(f"   F1 Score:  {metrics['f1']:.4f}")
        print(f"   AUC:       {metrics['auc']:.4f}")
        
        return metrics
    
    def get_feature_importance(self, feature_names):
        """í”¼ì²˜ ì¤‘ìš”ë„ ì¶”ì¶œ"""
        importance_df = pd.DataFrame({'feature': feature_names})
        
        for name, model in self.models.items():
            if hasattr(model, 'feature_importances_'):
                importance_df[f'{name}_importance'] = model.feature_importances_
        
        # í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
        imp_cols = [c for c in importance_df.columns if 'importance' in c]
        importance_df['avg_importance'] = importance_df[imp_cols].mean(axis=1)
        importance_df = importance_df.sort_values('avg_importance', ascending=False)
        
        return importance_df
    
    def save_models(self, suffix=''):
        """ëª¨ë¸ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        for name, model in self.models.items():
            path = f'{self.model_dir}/{name}_{timestamp}{suffix}.pkl'
            joblib.dump(model, path)
            print(f"ğŸ’¾ {name} ì €ì¥: {path}")
        
        # Label Encoder ì €ì¥
        le_path = f'{self.model_dir}/label_encoder_{timestamp}{suffix}.pkl'
        joblib.dump(self.label_encoder, le_path)
        print(f"ğŸ’¾ LabelEncoder ì €ì¥: {le_path}")
    
    def run(self):
        """TimeSeriesSplit êµì°¨ ê²€ì¦ í•™ìŠµ"""
        print("=" * 60)
        print(f"ğŸš€ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ (TimeSeries {self.n_splits}-Fold)")
        print("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë“œ
        df = self.load_data()
        
        # 2. í”¼ì²˜ ì¤€ë¹„
        X, y, dates, stock_codes = self.prepare_features(df)
        
        # ë‚ ì§œ ê¸°ì¤€ ì •ë ¬ (TimeSeriesSplitì„ ìœ„í•´)
        df_sorted = df.sort_values('ë‚ ì§œ').reset_index(drop=True)
        X = X.loc[df_sorted.index].reset_index(drop=True)
        y = y[df_sorted.index]
        dates = dates[df_sorted.index]
        
        X_np = X.values  # numpyë¡œ ë³€í™˜
        
        # K-Fold ê²°ê³¼ ì €ì¥
        fold_results = {
            'xgboost': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'auc': []},
            'catboost': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'auc': []},
            'ensemble': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'auc': []}
        }
        
        # 3. TimeSeriesSplit êµì°¨ ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=self.n_splits)
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_np), 1):
            print(f"\n{'='*60}")
            print(f"ğŸ“‚ Fold {fold}/{self.n_splits}")
            print(f"{'='*60}")
            
            X_train, X_val = X_np[train_idx], X_np[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]
            
            # ë‚ ì§œ ë²”ìœ„ ì¶œë ¥
            train_dates = dates[train_idx]
            val_dates = dates[val_idx]
            print(f"   í•™ìŠµ: {pd.Timestamp(train_dates[0]).date()} ~ {pd.Timestamp(train_dates[-1]).date()} ({len(X_train):,}ê±´)")
            print(f"   ê²€ì¦: {pd.Timestamp(val_dates[0]).date()} ~ {pd.Timestamp(val_dates[-1]).date()} ({len(X_val):,}ê±´)")
            print(f"   í•™ìŠµ í´ë˜ìŠ¤ ë¹„ìœ¨ - 0: {sum(y_train==0):,}, 1: {sum(y_train==1):,}")
            
            # ëª¨ë¸ ì´ˆê¸°í™”
            self.models = {}
            
            # ëª¨ë¸ í•™ìŠµ
            self.train_xgboost(X_train, y_train, X_val, y_val)
            self.train_catboost(X_train, y_train, X_val, y_val)
            
            # ê°œë³„ ëª¨ë¸ í‰ê°€
            print(f"\nğŸ“Š Fold {fold} ì„±ëŠ¥:")
            for name, model in self.models.items():
                y_pred = model.predict(X_val)
                y_proba = model.predict_proba(X_val)[:, 1]
                metrics = self.evaluate(y_val, y_pred, y_proba, name.upper())
                for k, v in metrics.items():
                    fold_results[name][k].append(v)
            
            # ì•™ìƒë¸” í‰ê°€
            ensemble_pred, ensemble_proba, _, _ = self.ensemble_predict(X_val, method='soft')
            ensemble_metrics = self.evaluate(y_val, ensemble_pred, ensemble_proba, "ENSEMBLE")
            for k, v in ensemble_metrics.items():
                fold_results['ensemble'][k].append(v)
        
        # 4. ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
        print(f"\n{'='*60}")
        print(f"ğŸ¯ ìµœì¢… ëª¨ë¸ í•™ìŠµ (ì „ì²´ ë°ì´í„°)")
        print(f"{'='*60}")
        
        self.models = {}
        # ì „ì²´ ë°ì´í„°ì˜ 80%ë¡œ í•™ìŠµ, 20%ë¡œ ê²€ì¦ (ìµœì¢… ëª¨ë¸ìš©)
        split_idx = int(len(X_np) * 0.8)
        X_train_final, X_val_final = X_np[:split_idx], X_np[split_idx:]
        y_train_final, y_val_final = y[:split_idx], y[split_idx:]
        
        self.train_xgboost(X_train_final, y_train_final, X_val_final, y_val_final)
        self.train_catboost(X_train_final, y_train_final, X_val_final, y_val_final)
        
        # 5. K-Fold í‰ê·  ê²°ê³¼
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ {self.n_splits}-Fold êµì°¨ ê²€ì¦ ê²°ê³¼ (í‰ê·  Â± í‘œì¤€í¸ì°¨)")
        print(f"{'='*60}")
        
        avg_results = {}
        for model_name, metrics in fold_results.items():
            avg_results[model_name] = {}
            print(f"\nğŸ“ˆ {model_name.upper()}:")
            for metric_name, values in metrics.items():
                mean_val = np.mean(values)
                std_val = np.std(values)
                avg_results[model_name][metric_name] = mean_val
                print(f"   {metric_name:10s}: {mean_val:.4f} Â± {std_val:.4f}")
        
        # 6. í”¼ì²˜ ì¤‘ìš”ë„
        print(f"\n{'='*60}")
        print(f"ğŸ“Œ í”¼ì²˜ ì¤‘ìš”ë„ (Top 10)")
        print(f"{'='*60}")
        
        importance_df = self.get_feature_importance(X.columns.tolist())
        print(importance_df[['feature', 'avg_importance']].head(10).to_string(index=False))
        
        # 7. ëª¨ë¸ ì €ì¥
        print(f"\n{'='*60}")
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥")
        print(f"{'='*60}")
        
        self.save_models()
        importance_df.to_csv(f'{self.model_dir}/feature_importance.csv', index=False)
        
        print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
        
        return avg_results, importance_df


def main():
    # ë°ì´í„° ê²½ë¡œ
    data_path = '_data/merged_all_stocks_20260131.csv'
    
    # í•™ìŠµ ì‹¤í–‰
    # target_threshold: 1% ì´ìƒ ìƒìŠ¹/í•˜ë½ë§Œ í•™ìŠµ
    # Noneìœ¼ë¡œ ì„¤ì •í•˜ë©´ ê¸°ì¡´ íƒ€ê²Ÿ ì‚¬ìš©
    trainer = StockEnsembleTrainer(
        data_path, 
        n_splits=5,
        lag_days=[1, 2, 3, 5, 10],
        target_threshold=0.01  # 1% ì„ê³„ê°’
    )
    metrics, importance = trainer.run()
    
    print("\nâœ… í•™ìŠµ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

