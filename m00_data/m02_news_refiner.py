import pandas as pd
import os
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from tqdm import tqdm
import warnings
from datetime import datetime, timedelta
import numpy as np
import math

warnings.filterwarnings('ignore')

# ---------------------------------------------------------
# FinBERTë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ (í…Œë§ˆ í•„í„°ë§ ê¸°ëŠ¥ ì¶”ê°€)
# ---------------------------------------------------------

def load_sentiment_model():
    """
    í•œêµ­ì–´ ê¸ˆìœµ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ
    """
    print("í•œêµ­ì–´ ê¸ˆìœµ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë”© ì¤‘...")
    
    # snunlp/KR-FinBert-SC ê°€ ê¸ˆìœµ íŠ¹í™”ë¼ ê°€ì¥ ì¶”ì²œë¨
    model_candidates = [
        "snunlp/KR-FinBert-SC",  
        "monologg/kofinbert",
        "beomi/KcELECTRA-base",
        "ProsusAI/finbert", 
    ]
    
    tokenizer = None
    model = None
    
    for model_name in model_candidates:
        try:
            print(f"  ì‹œë„ ì¤‘: {model_name}")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSequenceClassification.from_pretrained(model_name)
            model.eval()
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name}")
            return tokenizer, model
        except Exception as e:
            print(f"  âš ï¸ {model_name} ë¡œë”© ì‹¤íŒ¨: {e}")
            continue
    
    print("âš ï¸ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨. í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„± ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    return None, None

def analyze_sentiment_keyword_based(text):
    """
    í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì„± ë¶„ì„ (ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë°±ì—…ìš©)
    """
    if pd.isna(text) or text == '' or text is None:
        return 'Neutral', 0.0, 0.0, 1.0
    
    text_lower = str(text).lower()
    
    positive_keywords = [
        'ìƒìŠ¹', 'ì¦ê°€', 'ì„±ì¥', 'í˜¸ì¡°', 'ê°œì„ ', 'í™•ëŒ€', 'ìƒí–¥', 'ê°•ì„¸', 'ê¸‰ë“±', 'ë°˜ë“±',
        'ìˆ˜ìµ', 'ì´ìµ', 'ì‹¤ì ', 'í˜¸ì¬', 'ê¸ì •', 'ë‚™ê´€', 'ê¸°ëŒ€', 'ëŒíŒŒ', 'ì‹ ê³ ê°€', 'ìµœê³ ê°€', 
        'ìµœëŒ€', 'ê¸°ë¡', 'ë‹¬ì„±', 'íˆ¬ì', 'í™•ì¥', 'ì§„ì¶œ', 'ê³µê¸‰', 'ìˆ˜ìš”'
    ]
    
    negative_keywords = [
        'í•˜ë½', 'ê°ì†Œ', 'ì¶•ì†Œ', 'ì•…í™”', 'í•˜í–¥', 'ì•½ì„¸', 'ê¸‰ë½', 'í­ë½', 'ì¶”ë½',
        'ì†ì‹¤', 'ì†í•´', 'ë¶€ì§„', 'ì•…ì¬', 'ë¶€ì •', 'ë¹„ê´€', 'ìš°ë ¤', 'í•˜íšŒ', 'ë¯¸ë‹¬', 
        'ë¶€ì¡±', 'ìœ„ì¶•', 'í›„í‡´', 'í‡´ë³´', 'ê²½ê³ ', 'ìœ„í—˜', 'ë¦¬ìŠ¤í¬', 'ë¶ˆì•ˆ'
    ]
    
    pos_count = sum(1 for keyword in positive_keywords if keyword in text_lower)
    neg_count = sum(1 for keyword in negative_keywords if keyword in text_lower)
    
    total_keywords = pos_count + neg_count
    if total_keywords == 0:
        return 'Neutral', 0.0, 0.0, 1.0
    
    pos_prob = min(0.9, 0.5 + (pos_count / max(total_keywords, 1)) * 0.4)
    neg_prob = min(0.9, 0.5 + (neg_count / max(total_keywords, 1)) * 0.4)
    neu_prob = 1.0 - pos_prob - neg_prob
    
    total = pos_prob + neg_prob + neu_prob
    pos_prob /= total
    neg_prob /= total
    neu_prob /= total
    
    if pos_prob > neg_prob and pos_prob > neu_prob:
        sentiment = 'Positive'
    elif neg_prob > pos_prob and neg_prob > neu_prob:
        sentiment = 'Negative'
    else:
        sentiment = 'Neutral'
    
    return sentiment, pos_prob, neg_prob, neu_prob

def analyze_sentiment_batch(texts, tokenizer, model, device='cpu', batch_size=32):
    """
    ë°°ì¹˜ ë‹¨ìœ„ ê°ì„± ë¶„ì„ (ì†ë„ ìµœì í™” & ë¼ë²¨ ìë™ ë§¤í•‘)
    """
    results = {'sentiment': [], 'positive_prob': [], 'negative_prob': [], 'neutral_prob': []}
    clean_texts = [str(t) if pd.notna(t) and t != '' else '' for t in texts]
    id2label = model.config.id2label
    num_batches = math.ceil(len(clean_texts) / batch_size)
    
    for i in tqdm(range(num_batches), desc="   ë”¥ëŸ¬ë‹ ë¶„ì„ ì¤‘", leave=False):
        batch_texts = clean_texts[i*batch_size : (i+1)*batch_size]
        
        if all(t == '' for t in batch_texts):
            for _ in batch_texts:
                results['sentiment'].append('Neutral')
                results['positive_prob'].append(0.0)
                results['negative_prob'].append(0.0)
                results['neutral_prob'].append(1.0)
            continue

        try:
            inputs = tokenizer(batch_texts, return_tensors="pt", truncation=True, padding=True, max_length=512)
            if device == 'cuda':
                inputs = {k: v.to(device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=-1).cpu().numpy()
            
            for j, prob_arr in enumerate(probs):
                if batch_texts[j] == '':
                    results['sentiment'].append('Neutral')
                    results['positive_prob'].append(0.0)
                    results['negative_prob'].append(0.0)
                    results['neutral_prob'].append(1.0)
                    continue

                score_map = {'positive': 0.0, 'negative': 0.0, 'neutral': 0.0}
                if id2label:
                    for idx, score in enumerate(prob_arr):
                        label_name = str(id2label[idx]).lower()
                        if 'pos' in label_name: score_map['positive'] = float(score)
                        elif 'neg' in label_name: score_map['negative'] = float(score)
                        elif 'neu' in label_name: score_map['neutral'] = float(score)
                        elif "finbert" in model.name_or_path.lower(): # snunlp ì˜ˆì™¸ì²˜ë¦¬
                            if idx == 0: score_map['negative'] = float(score)
                            elif idx == 1: score_map['neutral'] = float(score)
                            elif idx == 2: score_map['positive'] = float(score)
                else:
                    if len(prob_arr) == 3:
                        score_map['negative'] = float(prob_arr[0])
                        score_map['neutral'] = float(prob_arr[1])
                        score_map['positive'] = float(prob_arr[2])
                    elif len(prob_arr) == 2:
                        score_map['positive'] = float(prob_arr[0])
                        score_map['negative'] = float(prob_arr[1])

                best_label = max(score_map, key=score_map.get).capitalize()
                results['sentiment'].append(best_label)
                results['positive_prob'].append(score_map['positive'])
                results['negative_prob'].append(score_map['negative'])
                results['neutral_prob'].append(score_map['neutral'])
                
        except Exception as e:
            print(f"Batch Error: {e}")
            for _ in batch_texts:
                results['sentiment'].append('Neutral')
                results['positive_prob'].append(0.0)
                results['negative_prob'].append(0.0)
                results['neutral_prob'].append(1.0)

    return results

def calculate_effective_date(news_date, news_time=None):
    """
    15:25 ì´í›„ ë‰´ìŠ¤ëŠ” ë‹¤ìŒ ì˜ì—…ì¼ë¡œ ì²˜ë¦¬
    """
    if isinstance(news_date, str):
        try:
            if news_time and pd.notna(news_time): news_datetime = pd.to_datetime(f"{news_date} {news_time}")
            else: news_datetime = pd.to_datetime(news_date)
        except: news_datetime = pd.to_datetime(news_date)
    elif isinstance(news_date, pd.Timestamp): news_datetime = news_date.to_pydatetime()
    elif isinstance(news_date, datetime): news_datetime = news_date
    elif hasattr(news_date, 'date'): news_datetime = datetime.combine(news_date, datetime.min.time())
    else: news_datetime = pd.to_datetime(news_date)
    
    if isinstance(news_datetime, datetime):
        if not hasattr(news_datetime, 'hour') or news_datetime.hour is None:
            news_datetime = news_datetime.replace(hour=12, minute=0, second=0)
    else:
        if pd.isna(news_datetime.hour) if hasattr(news_datetime, 'hour') else True:
            news_datetime = news_datetime.replace(hour=12, minute=0, second=0)
        news_datetime = news_datetime.to_pydatetime()
    
    market_close_hour, market_close_minute = 15, 25
    weekday = news_datetime.weekday()
    news_time = news_datetime.time()
    close_time = datetime.min.replace(hour=market_close_hour, minute=market_close_minute).time()
    
    if weekday >= 5: 
        effective_date = (news_datetime + timedelta(days=(7 - weekday))).date()
    elif news_time >= close_time:
        next_day = news_datetime + timedelta(days=1)
        if next_day.weekday() >= 5:
            effective_date = (next_day + timedelta(days=(7 - next_day.weekday()))).date()
        else:
            effective_date = next_day.date()
    else:
        effective_date = news_datetime.date()
    
    return effective_date

def process_news_files(data_dir, output_dir=None, theme_keywords=None):
    """
    theme_keywords: í•„í„°ë§í•  ì£¼ì œì–´ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ í•„í„°ë§ ì•ˆ í•¨)
    """
    tokenizer, model = load_sentiment_model()
    use_keyword_based = (tokenizer is None or model is None)
    
    device = 'cpu'
    if not use_keyword_based and torch.cuda.is_available():
        device = 'cuda'
        model = model.to(device)
        print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device} (GPU ê°€ì† í™œì„±í™”)")
    
    if output_dir is None: output_dir = data_dir
    
    news_files = []
    if os.path.isdir(data_dir):
        for file in os.listdir(data_dir):
            if file.startswith('NewsResult_with_sentiment_'): continue
            if file.endswith(('.xlsx', '.xls', '.csv')) and 'News' in file:
                news_files.append(os.path.join(data_dir, file))
    else:
        if not os.path.basename(data_dir).startswith('NewsResult_with_sentiment_'):
            news_files = [data_dir]
    
    if len(news_files) == 0:
        print("âš ï¸ ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    all_results = []
    
    for file_path in news_files:
        print(f"\n{'='*50}")
        print(f"ğŸ“„ íŒŒì¼ ë¡œë“œ: {os.path.basename(file_path)}")
        try:
            if file_path.endswith('.csv'): df = pd.read_csv(file_path)
            else: df = pd.read_excel(file_path)
            
            # --- í…ìŠ¤íŠ¸ ì»¬ëŸ¼ íƒìƒ‰ ---
            exclude_keywords = ['ì‹ë³„ì', 'id', 'identifier', 'ë²ˆí˜¸', 'number', 'ì½”ë“œ', 'code']
            text_columns = []
            for col in df.columns:
                col_lower = str(col).lower()
                if any(k in col_lower for k in ['title', 'ì œëª©', 'content', 'ë³¸ë¬¸']):
                    if not any(ek in col_lower for ek in exclude_keywords): text_columns.append(col)
            
            if not text_columns:
                for col in df.columns:
                    if df[col].dtype == 'object':
                        if len(str(df[col].iloc[0])) > 30: text_columns.append(col)

            # í…ìŠ¤íŠ¸ ê²°í•©
            if len(text_columns) == 1:
                df['combined_text'] = df[text_columns[0]].fillna('').astype(str)
            else:
                df['combined_text'] = df[text_columns].fillna('').astype(str).agg(' '.join, axis=1)
            
            original_count = len(df)
            
            # --- [í•µì‹¬] í…Œë§ˆ í‚¤ì›Œë“œ í•„í„°ë§ ---
            if theme_keywords and len(theme_keywords) > 0:
                print(f"ğŸ” í…Œë§ˆ í•„í„°ë§ ì ìš© ì¤‘... (í‚¤ì›Œë“œ: {theme_keywords})")
                
                # í‚¤ì›Œë“œê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ True
                # (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰í•˜ê¸° ìœ„í•´ lower() ì ìš©)
                def check_keywords(text):
                    text_lower = str(text).lower()
                    return any(k.lower() in text_lower for k in theme_keywords)
                
                mask = df['combined_text'].apply(check_keywords)
                df = df[mask].reset_index(drop=True)
                
                filtered_count = len(df)
                removed_count = original_count - filtered_count
                print(f"   ğŸ“‰ {original_count}ê±´ -> {filtered_count}ê±´ (í…Œë§ˆì™€ ë¬´ê´€í•œ {removed_count}ê±´ ì œì™¸ë¨)")
                
                if filtered_count == 0:
                    print("   âš ï¸ í•„í„°ë§ ê²°ê³¼ ë‚¨ì€ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ íŒŒì¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                    continue
            else:
                print("   â¡ï¸ í…Œë§ˆ í•„í„°ë§ ë¯¸ì ìš© (ëª¨ë“  ë‰´ìŠ¤ ë¶„ì„)")

            # --- ê°ì„± ë¶„ì„ ì‹¤í–‰ ---
            if use_keyword_based:
                sentiments, p_probs, n_probs, neu_probs = [], [], [], []
                for txt in tqdm(df['combined_text'], desc="   í‚¤ì›Œë“œ ë¶„ì„ ì¤‘"):
                    s, p, n, nu = analyze_sentiment_keyword_based(txt)
                    sentiments.append(s); p_probs.append(p); n_probs.append(n); neu_probs.append(nu)
                df['Sentiment'] = sentiments; df['Positive_Prob'] = p_probs
                df['Negative_Prob'] = n_probs; df['Neutral_Prob'] = neu_probs
            else:
                batch_res = analyze_sentiment_batch(df['combined_text'].tolist(), tokenizer, model, device, batch_size=32)
                df['Sentiment'] = batch_res['sentiment']
                df['Positive_Prob'] = batch_res['positive_prob']
                df['Negative_Prob'] = batch_res['negative_prob']
                df['Neutral_Prob'] = batch_res['neutral_prob']

            df['Sentiment_Score'] = df['Positive_Prob'] - df['Negative_Prob']
            
            # --- ì˜í–¥ì¼ì ê³„ì‚° ---
            date_col, time_col = None, None
            for col in df.columns:
                c_str = str(col).lower()
                if 'ì¼ì' in c_str or 'date' in c_str: date_col = col
                if 'ì‹œê°„' in c_str or 'time' in c_str: time_col = col
            
            if date_col:
                eff_dates = []
                for i in range(len(df)):
                    d = df[date_col].iloc[i]
                    t = df[time_col].iloc[i] if time_col else None
                    eff_dates.append(calculate_effective_date(d, t))
                df['Effective_Date'] = eff_dates
            
            # ì €ì¥
            base = os.path.basename(file_path).replace('NewsResult_with_sentiment_', 'NewsResult_')
            name, ext = os.path.splitext(base)
            out_file = os.path.join(output_dir, f"NewsResult_with_sentiment_{name}{ext}")
            
            if file_path.endswith('.csv'): df.to_csv(out_file, index=False, encoding='utf-8-sig')
            else: df.to_excel(out_file, index=False, engine='openpyxl')
            
            print(f"   âœ… ì €ì¥ ì™„ë£Œ: {out_file}")
            all_results.append(df)
            
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback; traceback.print_exc()

    return all_results

# ---------------------------------------------------------
# ì‹¤í–‰ ì„¤ì •
# ---------------------------------------------------------
if __name__ == "__main__":
    news_data_dir = "/home/jhkim/01_dev/03_stock_market_price_expectation/_data/01_news"
    output_dir = "/home/jhkim/01_dev/03_stock_market_price_expectation/_data/03_refined_news"
    
    # -----------------------------------------------------
    # [í˜•ë‹˜, ì—¬ê¸°ë§Œ ìˆ˜ì •í•˜ë©´ ë¼!]
    # ë¶„ì„í•˜ê³  ì‹¶ì€ í…Œë§ˆ(ê´€ì‹¬ ë¶„ì•¼)ì˜ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì¤˜.
    # ì˜ˆ: ['AI', 'ì¸ê³µì§€ëŠ¥', 'ë°˜ë„ì²´', 'ì‚¼ì„±ì „ì']
    # ë§Œì•½ ëª¨ë“  ë‰´ìŠ¤ë¥¼ ë‹¤ ë³´ê³  ì‹¶ìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¹„ì›Œë‘¬: []
    # -----------------------------------------------------
    # -----------------------------------------------------
    # [AI & ë¯¸ë˜ì‚°ì—… í…Œë§ˆ í‚¤ì›Œë“œ 100ì„ ]
    # -----------------------------------------------------
    THEME_KEYWORDS = [
        # 1. í•µì‹¬ í‚¤ì›Œë“œ (AI & ì†Œí”„íŠ¸ì›¨ì–´)
        'AI', 'ì¸ê³µì§€ëŠ¥', 'ìƒì„±í˜•', 'ìƒì„±í˜•AI', 'GenAI', 
        'ì±—ë´‡', 'Chatbot', 'ChatGPT', 'GPT', 'LLM', 'ê±°ëŒ€ì–¸ì–´ëª¨ë¸',
        'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì•Œê³ ë¦¬ì¦˜', 'ë¹…ë°ì´í„°', 'ë°ì´í„°ì„¼í„°', 
        'í´ë¼ìš°ë“œ', 'Cloud', 'SaaS', 'PaaS', 'API',
        'ì‹ ê²½ë§', 'NPU', 'ë¹„ì „', 'ìŒì„±ì¸ì‹', 'ìì—°ì–´ì²˜ë¦¬', 'NLP',
        
        # 2. êµ­ë‚´ ëŒ€í‘œ í”Œë«í¼ & AI ëŒ€ì¥ì£¼
        'ë„¤ì´ë²„', 'NAVER', 'í•˜ì´í¼í´ë¡œë°”', 'HyperClova', 'ì¹˜ì§€ì§',
        'ì¹´ì¹´ì˜¤', 'Kakao', 'ì¹´ì¹´ì˜¤ë¸Œë ˆì¸', 'KoGPT', 
        'ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤', 'LG CNS', 'SK C&C', 
        
        # 3. AI ë°˜ë„ì²´ & í•˜ë“œì›¨ì–´ (ê°€ì¥ ì¤‘ìš”)
        'ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤', 'HBM', 'HBM3', 'HBM3E', 'CXL', 'PIM',
        'ë°˜ë„ì²´', 'ë©”ëª¨ë¦¬', 'ì‹œìŠ¤í…œë°˜ë„ì²´', 'íŒŒìš´ë“œë¦¬', 'íŒ¨í‚¤ì§•',
        'GPU', 'ì—”ë¹„ë””ì•„', 'NVIDIA', 'AMD', 'ì¸í…”', 'ARM',
        'ì˜¨ë””ë°”ì´ìŠ¤', 'On-Device', 'ì—£ì§€ì»´í“¨íŒ…', 'ìŠ¤ëƒ…ë“œë˜ê³¤', 'ì—‘ì‹œë…¸ìŠ¤',
        'í•œë¯¸ë°˜ë„ì²´', 'HPSP', 'ì´ìˆ˜í˜íƒ€ì‹œìŠ¤', 'ë¦¬ë…¸ê³µì—…', 'ê³ ì˜', 'ì£¼ì„±ì—”ì§€ë‹ˆì–´ë§',
        
        # 4. ë¡œë´‡ & ììœ¨ì£¼í–‰ (AIì˜ ì†ê³¼ ë°œ)
        'ë¡œë´‡', 'Robot', 'íœ´ë¨¸ë…¸ì´ë“œ', 'í˜‘ë™ë¡œë´‡', 'ì‚°ì—…ìš©ë¡œë´‡', 
        'ë ˆì¸ë³´ìš°ë¡œë³´í‹±ìŠ¤', 'ë‘ì‚°ë¡œë³´í‹±ìŠ¤', 'ìœ ì§„ë¡œë´‡', 'í‹°ë¡œë³´í‹±ìŠ¤',
        'ììœ¨ì£¼í–‰', 'ëª¨ë¹Œë¦¬í‹°', 'SDV', 'í˜„ëŒ€ì˜¤í† ì—ë²„', 'ìŠ¤ë§ˆíŠ¸ì¹´',
        'í˜„ëŒ€ì°¨', 'ê¸°ì•„', 'í…ŒìŠ¬ë¼', 'Tesla',
        
        # 5. ì˜ë£Œ AI & ë°”ì´ì˜¤ (í•«í•œ ì„¹í„°)
        'ì˜ë£ŒAI', 'ë””ì§€í„¸í—¬ìŠ¤ì¼€ì–´', 'ì‹ ì•½ê°œë°œ', 'ìœ ì „ì²´',
        'ë£¨ë‹›', 'ë·°ë…¸', 'ì œì´ì—˜ì¼€ì´', 'ë”¥ë…¸ì´ë“œ', 
        
        # 6. ë©”íƒ€ë²„ìŠ¤ & í†µì‹  & ë³´ì•ˆ
        'ë©”íƒ€ë²„ìŠ¤', 'XR', 'VR', 'AR', 'ë””ì§€í„¸íŠ¸ìœˆ',
        'í†µì‹ ', '5G', '6G', 'SKí…”ë ˆì½¤', 'KT', 'LGìœ í”ŒëŸ¬ìŠ¤',
        'ë³´ì•ˆ', 'ì‚¬ì´ë²„ë³´ì•ˆ', 'ì •ë³´ë³´ì•ˆ', 'ì•ˆë©', 'ìƒŒì¦ˆë©', 'ëª¨ë‹ˆí„°ë©',
        
        # 7. AI ê´€ë ¨ ê¸€ë¡œë²Œ ë¹…í…Œí¬ (êµ­ë‚´ ë‰´ìŠ¤ì— ìì£¼ ì–¸ê¸‰ë¨)
        'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸', 'Microsoft', 'MS', 'ì˜¤í”ˆAI', 'OpenAI',
        'êµ¬ê¸€', 'Google', 'ì œë¯¸ë‚˜ì´', 'Gemini', 
        'ì• í”Œ', 'Apple', 'ë¹„ì „í”„ë¡œ', 'ì•„ë§ˆì¡´', 'AWS', 'ë©”íƒ€', 'Meta',
        
        # 8. ê¸°íƒ€ ê´€ë ¨ ì£¼ìš” ìš©ì–´
        'ìŠ¤ë§ˆíŠ¸íŒ©í† ë¦¬', 'ê³µì •ìë™í™”', 'ìˆ˜ìœ¨', 'ê³µê¸‰ë§', 'ë°ì´í„°ëŒ',
        'ë””ì§€í„¸ì „í™˜', 'DX', 'í•€í…Œí¬', 'STO', 'ë¸”ë¡ì²´ì¸'
    ]
    
    print("="*60)
    print("ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œì‘ (í…Œë§ˆ í•„í„°ë§ í¬í•¨)")
    print("="*60)
    
    results = process_news_files(news_data_dir, output_dir, theme_keywords=THEME_KEYWORDS)
    
    print("\n" + "="*60)
    print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    print("="*60)