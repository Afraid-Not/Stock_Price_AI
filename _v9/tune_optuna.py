"""
Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- XGBoost + LightGBM + CatBoost ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰
"""
import pandas as pd
import numpy as np
import random
import warnings
import argparse
import optuna
from optuna.samplers import TPESampler
warnings.filterwarnings('ignore')

from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import roc_auc_score, f1_score
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostClassifier
import joblib
import os
from datetime import datetime

# 9ê°œ ì¢…ëª© ì½”ë“œ
TARGET_STOCKS = [
    '005930', '000660', '035420', '035720', '006400',
    '066570', '034220', '018260', '030200'
]


def set_seed(seed: int):
    """ëª¨ë“  ëžœë¤ ì‹œë“œ ê³ ì •"""
    random.seed(seed)
    np.random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)


class OptunaOptimizer:
    def __init__(self, data_path, n_splits=5, lag_days=[1, 3, 5],
                 target_threshold=0.01, seed=42, n_trials=50):
        self.data_path = data_path
        self.n_splits = n_splits
        self.lag_days = lag_days
        self.target_threshold = target_threshold
        self.seed = seed
        self.n_trials = n_trials
        self.label_encoder = LabelEncoder()
        
        set_seed(seed)
        
        # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        self.X, self.y = self.prepare_data()
        
    def prepare_data(self):
        """ë°ì´í„° ì¤€ë¹„"""
        print("ðŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
        df = pd.read_csv(self.data_path)
        
        # stock_code ë¬¸ìžì—´ ë³€í™˜
        df['stock_code'] = df['stock_code'].astype(str).str.zfill(6)
        
        # 9ê°œ ì¢…ëª© í•„í„°
        df = df[df['stock_code'].isin(TARGET_STOCKS)].copy()
        
        # ë‚ ì§œ ì²˜ë¦¬ - âš ï¸ TimeSeriesSplitì„ ìœ„í•´ ë‚ ì§œ ìš°ì„  ì •ë ¬!
        df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'].astype(str), format='%Y%m%d', errors='coerce')
        df = df.dropna(subset=['ë‚ ì§œ'])
        df = df.sort_values(['ë‚ ì§œ', 'stock_code']).reset_index(drop=True)
        
        print(f"   ì›ë³¸: {len(df):,}ê±´")
        
        # íƒ€ê²Ÿ ìž¬ì •ì˜
        if 'next_rtn' in df.columns:
            df = df.dropna(subset=['next_rtn'])
            df_up = df[df['next_rtn'] >= self.target_threshold].copy()
            df_up['target'] = 1
            df_down = df[df['next_rtn'] <= -self.target_threshold].copy()
            df_down['target'] = 0
            df = pd.concat([df_up, df_down], ignore_index=True)
            df = df.sort_values(['ë‚ ì§œ', 'stock_code']).reset_index(drop=True)
            df = df.drop(columns=['next_rtn'])
        
        print(f"   í•„í„°ë§ í›„: {len(df):,}ê±´")
        
        # Lag í”¼ì²˜
        df = self.add_lag_features(df)
        print(f"   Lag ì¶”ê°€ í›„: {len(df):,}ê±´")
        
        # í”¼ì²˜ ì¶”ì¶œ
        exclude_cols = ['ë‚ ì§œ', 'target', 'stock_code', 'stock_code_encoded',
                        'ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ê±°ëž˜ëŸ‰', 'ê±°ëž˜ëŒ€ê¸ˆ',
                        'stock_name', 'next_rtn']
        feature_cols = [c for c in df.columns if c not in exclude_cols]
        
        # stock_code ì¸ì½”ë”©
        df['stock_code_encoded'] = self.label_encoder.fit_transform(df['stock_code'])
        feature_cols.append('stock_code_encoded')
        
        X = df[feature_cols].values
        y = df['target'].values
        
        print(f"   í”¼ì²˜ ìˆ˜: {len(feature_cols)}")
        print(f"   í´ëž˜ìŠ¤: 0={sum(y==0):,}, 1={sum(y==1):,}")
        
        return X, y
    
    def add_lag_features(self, df):
        """Lag í”¼ì²˜ ì¶”ê°€"""
        base_features = [
            'open_gap', 'high_ratio', 'low_ratio', 'volatility',
            'ê°œì¸_ì²´ê²°ê°•ë„', 'ì™¸êµ­ì¸_ì²´ê²°ê°•ë„', 'ê¸°ê´€ê³„_ì²´ê²°ê°•ë„',
            'vol_ratio', 'rsi'
        ]
        base_features = [f for f in base_features if f in df.columns]
        
        lag_dfs = []
        for stock_code in df['stock_code'].unique():
            stock_df = df[df['stock_code'] == stock_code].copy()
            stock_df = stock_df.sort_values('ë‚ ì§œ')
            
            for lag in self.lag_days:
                for feat in base_features:
                    stock_df[f'{feat}_lag{lag}'] = stock_df[feat].shift(lag)
            
            lag_dfs.append(stock_df)
        
        df = pd.concat(lag_dfs, ignore_index=True)
        df = df.dropna()
        # âš ï¸ TimeSeriesSplitì„ ìœ„í•´ ë‚ ì§œ ìš°ì„  ì •ë ¬!
        df = df.sort_values(['ë‚ ì§œ', 'stock_code']).reset_index(drop=True)
        return df
    
    def objective_xgb(self, trial):
        """XGBoost ëª©ì  í•¨ìˆ˜"""
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'random_state': self.seed,
            'eval_metric': 'auc',
            'early_stopping_rounds': 50
        }
        
        tscv = TimeSeriesSplit(n_splits=self.n_splits)
        scores = []
        
        for train_idx, val_idx in tscv.split(self.X):
            X_train, X_val = self.X[train_idx], self.X[val_idx]
            y_train, y_val = self.y[train_idx], self.y[val_idx]
            
            model = xgb.XGBClassifier(**params)
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                verbose=False
            )
            
            y_pred = model.predict(X_val)
            f1 = f1_score(y_val, y_pred)
            scores.append(f1)
        
        return np.mean(scores)
    
    def objective_lgb(self, trial):
        """LightGBM ëª©ì  í•¨ìˆ˜"""
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'num_leaves': trial.suggest_int('num_leaves', 20, 150),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'random_state': self.seed,
            'verbose': -1
        }
        
        tscv = TimeSeriesSplit(n_splits=self.n_splits)
        scores = []
        
        for train_idx, val_idx in tscv.split(self.X):
            X_train, X_val = self.X[train_idx], self.X[val_idx]
            y_train, y_val = self.y[train_idx], self.y[val_idx]
            
            model = lgb.LGBMClassifier(**params)
            model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                callbacks=[lgb.early_stopping(50, verbose=False)]
            )
            
            y_pred = model.predict(X_val)
            f1 = f1_score(y_val, y_pred)
            scores.append(f1)
        
        return np.mean(scores)
    
    def objective_cat(self, trial):
        """CatBoost ëª©ì  í•¨ìˆ˜"""
        params = {
            'iterations': trial.suggest_int('iterations', 100, 1000),
            'depth': trial.suggest_int('depth', 3, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),
            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
            'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True),
            'random_state': self.seed,
            'eval_metric': 'AUC',
            'early_stopping_rounds': 50,
            'verbose': False
        }
        
        tscv = TimeSeriesSplit(n_splits=self.n_splits)
        scores = []
        
        for train_idx, val_idx in tscv.split(self.X):
            X_train, X_val = self.X[train_idx], self.X[val_idx]
            y_train, y_val = self.y[train_idx], self.y[val_idx]
            
            model = CatBoostClassifier(**params)
            model.fit(
                X_train, y_train,
                eval_set=(X_val, y_val)
            )
            
            y_pred = model.predict(X_val)
            f1 = f1_score(y_val, y_pred)
            scores.append(f1)
        
        return np.mean(scores)
    
    def run(self):
        """ìµœì í™” ì‹¤í–‰"""
        print("\n" + "=" * 60)
        print("ðŸ”¬ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
        print("=" * 60)
        print(f"ì‹œë“œ: {self.seed}")
        print(f"ì‹œí–‰ íšŸìˆ˜: {self.n_trials}")
        print(f"CV Folds: {self.n_splits}")
        
        results = {}
        
        # XGBoost ìµœì í™”
        print("\n" + "-" * 40)
        print("ðŸ”µ XGBoost íŠœë‹ ì¤‘...")
        print("-" * 40)
        
        study_xgb = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=self.seed)
        )
        study_xgb.optimize(
            self.objective_xgb, 
            n_trials=self.n_trials,
            show_progress_bar=True
        )
        
        results['xgboost'] = {
            'best_params': study_xgb.best_params,
            'best_f1': study_xgb.best_value
        }
        
        print(f"\nâœ… XGBoost ìµœì  F1: {study_xgb.best_value:.4f}")
        print("   ìµœì  íŒŒë¼ë¯¸í„°:")
        for k, v in study_xgb.best_params.items():
            print(f"      {k}: {v}")
        
        # LightGBM ìµœì í™”
        print("\n" + "-" * 40)
        print("ðŸŸ¢ LightGBM íŠœë‹ ì¤‘...")
        print("-" * 40)
        
        study_lgb = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=self.seed)
        )
        study_lgb.optimize(
            self.objective_lgb, 
            n_trials=self.n_trials,
            show_progress_bar=True
        )
        
        results['lightgbm'] = {
            'best_params': study_lgb.best_params,
            'best_f1': study_lgb.best_value
        }
        
        print(f"\nâœ… LightGBM ìµœì  F1: {study_lgb.best_value:.4f}")
        print("   ìµœì  íŒŒë¼ë¯¸í„°:")
        for k, v in study_lgb.best_params.items():
            print(f"      {k}: {v}")
        
        # CatBoost ìµœì í™”
        print("\n" + "-" * 40)
        print("ðŸŸ¡ CatBoost íŠœë‹ ì¤‘...")
        print("-" * 40)
        
        study_cat = optuna.create_study(
            direction='maximize',
            sampler=TPESampler(seed=self.seed)
        )
        study_cat.optimize(
            self.objective_cat,
            n_trials=self.n_trials,
            show_progress_bar=True
        )
        
        results['catboost'] = {
            'best_params': study_cat.best_params,
            'best_f1': study_cat.best_value
        }
        
        print(f"\nâœ… CatBoost ìµœì  F1: {study_cat.best_value:.4f}")
        print("   ìµœì  íŒŒë¼ë¯¸í„°:")
        for k, v in study_cat.best_params.items():
            print(f"      {k}: {v}")
        
        # ê²°ê³¼ ì €ìž¥
        os.makedirs('tuning_results', exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        result_path = f"tuning_results/optuna_{timestamp}.pkl"
        joblib.dump(results, result_path)
        print(f"\nðŸ’¾ ê²°ê³¼ ì €ìž¥: {result_path}")
        
        # í…ìŠ¤íŠ¸ë¡œë„ ì €ìž¥
        txt_path = f"tuning_results/best_params_{timestamp}.txt"
        with open(txt_path, 'w') as f:
            f.write(f"Seed: {self.seed}\n")
            f.write(f"Trials: {self.n_trials}\n")
            f.write(f"Threshold: {self.target_threshold}\n\n")
            
            f.write("=" * 40 + "\n")
            f.write("XGBoost\n")
            f.write("=" * 40 + "\n")
            f.write(f"Best F1: {results['xgboost']['best_f1']:.4f}\n")
            for k, v in results['xgboost']['best_params'].items():
                f.write(f"{k}: {v}\n")
            
            f.write("\n" + "=" * 40 + "\n")
            f.write("LightGBM\n")
            f.write("=" * 40 + "\n")
            f.write(f"Best F1: {results['lightgbm']['best_f1']:.4f}\n")
            for k, v in results['lightgbm']['best_params'].items():
                f.write(f"{k}: {v}\n")
            
            f.write("\n" + "=" * 40 + "\n")
            f.write("CatBoost\n")
            f.write("=" * 40 + "\n")
            f.write(f"Best F1: {results['catboost']['best_f1']:.4f}\n")
            for k, v in results['catboost']['best_params'].items():
                f.write(f"{k}: {v}\n")
        
        print(f"ðŸ’¾ íŒŒë¼ë¯¸í„° ì €ìž¥: {txt_path}")
        
        return results


def main():
    parser = argparse.ArgumentParser(description="Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    parser.add_argument("--data", type=str, default="_data/merged_with_macro.csv")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--n_trials", type=int, default=50, help="ì‹œí–‰ íšŸìˆ˜")
    parser.add_argument("--n_splits", type=int, default=5, help="CV Fold ìˆ˜")
    parser.add_argument("--threshold", type=float, default=0.01)
    args = parser.parse_args()
    
    optimizer = OptunaOptimizer(
        data_path=args.data,
        n_splits=args.n_splits,
        target_threshold=args.threshold,
        seed=args.seed,
        n_trials=args.n_trials
    )
    
    results = optimizer.run()
    
    print("\n" + "=" * 60)
    print("ðŸŽ‰ íŠœë‹ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nðŸ“Š ìµœì¢… ê²°ê³¼:")
    print(f"   XGBoost F1: {results['xgboost']['best_f1']:.4f}")
    print(f"   LightGBM F1: {results['lightgbm']['best_f1']:.4f}")
    print(f"   CatBoost F1: {results['catboost']['best_f1']:.4f}")


if __name__ == "__main__":
    main()

