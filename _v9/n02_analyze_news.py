"""
ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
LLM(GPT) ë˜ëŠ” FinBERTë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê°ì„± ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
"""
import pandas as pd
import numpy as np
import os
import sys
import time
import argparse
from datetime import datetime
from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv('D:/stock/.env')


class NewsAnalyzer:
    """ë‰´ìŠ¤ ê°ì„± ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, method: str = "llm", model: str = None):
        """
        Args:
            method: "llm" (GPT) ë˜ëŠ” "finbert" (ë¡œì»¬)
            model: ëª¨ë¸ëª… (llmì¼ ë•Œë§Œ ì‚¬ìš©)
        """
        self.method = method
        self.model = model or "gpt-4o-mini"
        self.client = None
        
        if method == "llm":
            self._init_llm()
        elif method == "finbert":
            self._init_finbert()
    
    def _init_llm(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            from openai import OpenAI
            api_key = os.environ.get("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            self.client = OpenAI(api_key=api_key)
            print(f"âœ… OpenAI ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {self.model})")
        except ImportError:
            print("âŒ openai íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. pip install openai")
            raise
    
    def _init_finbert(self):
        """FinBERT ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            import torch
            from transformers import AutoTokenizer, AutoModelForSequenceClassification
            
            model_name = "snunlp/KR-FinBert-SC"
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            print(f"ğŸ”§ FinBERT ë¡œë”© ì¤‘... (ë””ë°”ì´ìŠ¤: {self.device})")
            
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model_bert = AutoModelForSequenceClassification.from_pretrained(model_name)
            self.model_bert.to(self.device)
            self.model_bert.eval()
            print("âœ… FinBERT ë¡œë”© ì™„ë£Œ")
        except ImportError:
            print("âŒ transformers íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. pip install transformers torch")
            raise
    
    def analyze_single_llm(self, title: str, stock_name: str = None) -> dict:
        """LLMìœ¼ë¡œ ë‹¨ì¼ ë‰´ìŠ¤ ë¶„ì„"""
        stock_context = f"'{stock_name}' ì¢…ëª©ì— ëŒ€í•œ " if stock_name else ""
        
        prompt = f"""ë‹¤ìŒì€ {stock_context}ë‰´ìŠ¤ ì œëª©ì…ë‹ˆë‹¤. ì£¼ê°€ì— ë¯¸ì¹  ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë‰´ìŠ¤ ì œëª©: "{title}"

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{"sentiment": 0.0, "impact": 3, "event_type": "ì¼ë°˜"}}

- sentiment: -1.0(ë§¤ìš°ë¶€ì •) ~ +1.0(ë§¤ìš°ê¸ì •)
- impact: 1(ë‚®ìŒ) ~ 5(ë†’ìŒ)
- event_type: ì‹¤ì /ë°°ë‹¹/ê³„ì•½/íˆ¬ì/ê·œì œ/ì‹œì¥/ì¼ë°˜"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=100
            )
            text = response.choices[0].message.content
            
            # JSON íŒŒì‹±
            import json
            start = text.find('{')
            end = text.rfind('}') + 1
            if start != -1 and end > start:
                result = json.loads(text[start:end])
                return {
                    'sentiment': max(-1, min(1, float(result.get('sentiment', 0)))),
                    'impact': max(1, min(5, int(result.get('impact', 3)))),
                    'event_type': result.get('event_type', 'ì¼ë°˜')
                }
        except Exception as e:
            pass
        
        return {'sentiment': 0.0, 'impact': 3, 'event_type': 'ì¼ë°˜'}
    
    def analyze_single_finbert(self, title: str) -> dict:
        """FinBERTë¡œ ë‹¨ì¼ ë‰´ìŠ¤ ë¶„ì„"""
        import torch
        
        if not title or pd.isna(title):
            return {'sentiment': 0.0, 'impact': 3, 'event_type': 'ì¼ë°˜'}
        
        try:
            inputs = self.tokenizer(
                str(title)[:512],
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=512
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model_bert(**inputs)
                probs = torch.softmax(outputs.logits, dim=-1)
                scores = probs[0].cpu().tolist()
            
            # ê°ì„± ì ìˆ˜ ê³„ì‚° (ê¸ì • - ë¶€ì •)
            sentiment = scores[2] - scores[0]  # ê¸ì • - ë¶€ì •
            
            return {
                'sentiment': round(sentiment, 4),
                'impact': 3,  # FinBERTëŠ” ì˜í–¥ë„ ì œê³µ ì•ˆí•¨
                'event_type': 'ì¼ë°˜'
            }
        except Exception as e:
            return {'sentiment': 0.0, 'impact': 3, 'event_type': 'ì¼ë°˜'}
    
    def analyze_dataframe(self, df: pd.DataFrame, title_col: str = "HTS_ê³µì‹œ_ì œëª©_ë‚´ìš©",
                          stock_name_col: str = "stock_name", delay: float = 0.3,
                          batch_size: int = 100) -> pd.DataFrame:
        """DataFrame ì „ì²´ ë¶„ì„"""
        
        print(f"\nğŸ“Š ê°ì„± ë¶„ì„ ì‹œì‘ (ë°©ì‹: {self.method})")
        print(f"   ì´ {len(df):,}ê±´")
        
        results = []
        total = len(df)
        
        for i, (_, row) in enumerate(df.iterrows()):
            title = row.get(title_col, '')
            stock_name = row.get(stock_name_col, '')
            
            if self.method == "llm":
                result = self.analyze_single_llm(title, stock_name)
                time.sleep(delay)
            else:
                result = self.analyze_single_finbert(title)
            
            results.append(result)
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if (i + 1) % batch_size == 0:
                pct = (i + 1) / total * 100
                print(f"   ì§„í–‰: {i + 1:,}/{total:,} ({pct:.1f}%)")
        
        # ê²°ê³¼ ì¶”ê°€
        df_result = df.copy()
        df_result['sentiment'] = [r['sentiment'] for r in results]
        df_result['impact'] = [r['impact'] for r in results]
        df_result['event_type'] = [r['event_type'] for r in results]
        
        # í†µê³„
        print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
        print(f"   í‰ê·  ê°ì„±: {df_result['sentiment'].mean():.4f}")
        print(f"   ê¸ì • ë‰´ìŠ¤: {len(df_result[df_result['sentiment'] > 0.3]):,}ê±´")
        print(f"   ë¶€ì • ë‰´ìŠ¤: {len(df_result[df_result['sentiment'] < -0.3]):,}ê±´")
        
        return df_result
    
    def aggregate_daily(self, df: pd.DataFrame, date_col: str = "ì‘ì„±ì¼ì",
                        stock_col: str = "stock_code") -> pd.DataFrame:
        """ì¢…ëª©ë³„/ë‚ ì§œë³„ ì§‘ê³„"""
        
        # ë‚ ì§œ í˜•ì‹ ë³€í™˜
        df['ë‚ ì§œ'] = df[date_col].astype(str).apply(
            lambda x: f"{x[:4]}-{x[4:6]}-{x[6:8]}" if len(x) == 8 and x.isdigit() else x
        )
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        df['weighted'] = df['sentiment'] * df['impact']
        
        # ì¢…ëª©ë³„/ë‚ ì§œë³„ ì§‘ê³„
        df_daily = df.groupby([stock_col, 'ë‚ ì§œ']).agg({
            'weighted': 'sum',
            'impact': 'sum',
            'sentiment': ['mean', 'count']
        }).reset_index()
        
        df_daily.columns = [stock_col, 'ë‚ ì§œ', 'weighted_sum', 'impact_sum', 
                           'sentiment_mean', 'news_count']
        
        # ê°€ì¤‘ í‰ê· 
        df_daily['news_sentiment'] = (df_daily['weighted_sum'] / df_daily['impact_sum']).round(4)
        df_daily['news_sentiment_simple'] = df_daily['sentiment_mean'].round(4)
        
        # ìµœì¢… ì»¬ëŸ¼ ì„ íƒ
        df_daily = df_daily[[stock_col, 'ë‚ ì§œ', 'news_sentiment', 'news_sentiment_simple', 'news_count']]
        df_daily = df_daily.sort_values([stock_col, 'ë‚ ì§œ']).reset_index(drop=True)
        
        print(f"\nğŸ“Š ì¼ë³„ ì§‘ê³„ ì™„ë£Œ")
        print(f"   ì´ {len(df_daily):,}ê±´ (ì¢…ëª©Ã—ë‚ ì§œ)")
        
        return df_daily


def analyze_all_news(input_path: str, output_dir: str = "_data/news",
                     method: str = "finbert", delay: float = 0.3):
    """ì „ì²´ ë‰´ìŠ¤ ê°ì„± ë¶„ì„"""
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ë°ì´í„° ë¡œë“œ
    print(f"ğŸ“‚ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ: {input_path}")
    df = pd.read_csv(input_path, encoding='utf-8-sig')
    print(f"   ì´ {len(df):,}ê±´")
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = NewsAnalyzer(method=method)
    
    # ë¶„ì„ ì‹¤í–‰
    df_analyzed = analyzer.analyze_dataframe(df, delay=delay)
    
    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    analyzed_path = input_path.replace('.csv', f'_analyzed_{method}.csv')
    df_analyzed.to_csv(analyzed_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {analyzed_path}")
    
    # ì¼ë³„ ì§‘ê³„
    df_daily = analyzer.aggregate_daily(df_analyzed)
    
    daily_path = f"{output_dir}/news_sentiment_daily.csv"
    df_daily.to_csv(daily_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ ì¼ë³„ ì§‘ê³„ ì €ì¥: {daily_path}")
    
    return df_daily


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ë‰´ìŠ¤ ê°ì„± ë¶„ì„")
    parser.add_argument("-i", "--input", type=str, required=True, help="ì…ë ¥ CSV íŒŒì¼")
    parser.add_argument("-o", "--output", type=str, default="_data/news", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--method", type=str, default="finbert", 
                        choices=["llm", "finbert"], help="ë¶„ì„ ë°©ì‹")
    parser.add_argument("--delay", type=float, default=0.3, help="API í˜¸ì¶œ ê°„ê²© (llm)")
    
    args = parser.parse_args()
    
    analyze_all_news(
        input_path=args.input,
        output_dir=args.output,
        method=args.method,
        delay=args.delay
    )

